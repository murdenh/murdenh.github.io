[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Diffusion based Generative Model\n\n\nIntutive explanation and mathematical theory with simple examples\n\n\n\n\nGenerative model\n\n\nDiffusion model\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nKo-Q&A bot on edge\n\n\nPolyglot-ko language model based Q&A bot\n\n\n\n\napps\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nLLama bot\n\n\nLLama language model based Q&A bot\n\n\n\n\napps\n\n\n\n\n\n\n\n\n\n\n\nJun 24, 2023\n\n\n\n\n\n\n  \n\n\n\n\nP2P Meta Video Chat\n\n\nWeb-based Video chat application  (with P2P and ML based features)\n\n\n\n\napps\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "P2P Meta Video Chat",
    "section": "",
    "text": "This is the first sample web-application\nA few notable features are:\n\nPeer-to-Peer video/data communication\nSimple demonstration of Metaverse concept with ML-based face tracking"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "about2.html",
    "href": "about2.html",
    "title": "About2",
    "section": "",
    "text": "About2 this blog"
  },
  {
    "objectID": "apps.html",
    "href": "apps.html",
    "title": "Apps",
    "section": "",
    "text": "P2P Meta Video Chat: Sample web-app to demonstrate P2P video chat with Metaverse feature"
  },
  {
    "objectID": "posts/first-app/index.html",
    "href": "posts/first-app/index.html",
    "title": "P2P Meta Video Chat",
    "section": "",
    "text": "P2P Meta Talk (Link)\n\n\n\nFeatures:\n\nPeer-to-Peer video chat application\n\nClient - Client direct video/text communication (without server)\nServerless broker for communication initiation\n\nAvatar mode to transform face/body into a robot-like image\nSimple demonstration of Metaverse-like concept\n\nWarp of a person to different digital space\n\n\n\n\nInterface:\n\n\n\n\n\n\n\n(a) First UI (before connection)\n\n\n\n\n\n\n\n(b) Second UI (after connection)\n\n\n\n\nFigure 1: Interface\n\n\n\nFirst UI\n\nCheck what is your friend’s room ID\n\n(If you want to talk with multiple friends, check the room ID of one of your friends)\n\nType in your nickname\nType in the room ID obtained from 1\nClick the button to connect\n\nSecond UI\n\nAfter your friend accepts your connection, you can type in any text message\nClick the button to send the text message to your friend\n\n\n\n\nSpecial function keys:\n\nInsert: Background on/off (Real world <-> Green background color)\nDelete: Avatar mode on/off\nScroll-Lock: Metaverse mode on/off  (Real world -> Metaverse mode 1 -> Metaverse mode 2 -> Real world)\nCurly brace keys {, }: Warping controls in Metaverse mode\n\n\n\n\nMetaverse mode 2"
  },
  {
    "objectID": "posts/second-app/index.html",
    "href": "posts/second-app/index.html",
    "title": "LLama bot",
    "section": "",
    "text": "LLama bot (Link)\n\n\n\nDetails:\n\nModel: Fine-tuned LLama (from Meta)\nFinetuning method: QLoRA + Q&A corpus data\nFinetuning infra: Single Nvidia GPU on laptop\nInference runtime: NodeJS + Docker\nInference infra: GCP Cloud Run (Serverless service)\n\n\n\nExamples:\n\nQuestion\n\n\n\n\n\n\n\n\n\n\n\n\nRequest + context (separated by 3 dashes ---)"
  },
  {
    "objectID": "posts/third-app/index.html",
    "href": "posts/third-app/index.html",
    "title": "Ko-Q&A bot on edge",
    "section": "",
    "text": "Ko-Q&A bot on browser runtime (Link)\n\n\n\nDetails:\n\nModel: Fine-tuned Polyglot (from EleutherAI)\nFinetuning method: QLoRA + Q&A corpus data\nFinetuning infra: Single Nvidia GPU on laptop\nInference runtime: ONNX-runtime + Web browser\nInference infra: Any device with free RAM space > 4G\n\n\n\nInterface:\n\n\nDownload the fine-tuned language model\nType in a simple question\nClick the button to ask\nThe progress-bar will show up after 3, until the bot answers\n\n\n\nExamples:\n\nSimple question (length is limited to max 80 tokens)"
  },
  {
    "objectID": "posts/diffusion-model/Untitled.html",
    "href": "posts/diffusion-model/Untitled.html",
    "title": "Diffusion based Generative Model",
    "section": "",
    "text": "Command to install dependency libraries\n# !pip3 install torch, numpy, matplotlib"
  },
  {
    "objectID": "posts/diffusion-model/Untitled.html#theoretical-process-in-diffusion-based-models",
    "href": "posts/diffusion-model/Untitled.html#theoretical-process-in-diffusion-based-models",
    "title": "Diffusion based Generative Model",
    "section": "Theoretical process in diffusion-based models",
    "text": "Theoretical process in diffusion-based models\n\nTo understand the diffusion model, it’s necessary to get a grasp on the diffusion process, a theoretical foundation of the model. In short, the diffusion process is a sequential process that transforms the data closer to a noise with a lot of slight-update steps. The idea of mapping data to a noise has some similarities with the other generative models.\n\\[\n\\begin{gathered}\nx_t = \\alpha_t x_{t-1} + \\beta_t \\epsilon_{t} \\\\\n\\tiny where \\ \\ \\alpha_t < 1, \\alpha_t^2 + \\beta_t^2 = 1,\\epsilon_{t} \\sim Normal(0, I) \\ \\ in \\ DDPM\n\\end{gathered}\n\\]\nThe equation above shows how the data \\(x\\) is transformed in each step. As the noise \\(\\epsilon\\) is added to the data \\(x\\) that is being scaled down (\\(\\alpha < 1\\)), \\(x_T\\) and \\(\\epsilon\\) will be indistinguishable from each other after a lot of steps T. This is called forward diffusion process.\nHowever, the generative model should be able to generate the data (not the noise), which means that reverse diffusion process is used to generate a sample from the noise distribution. (If \\(x_T\\) is viewed as a latent variable, it is somewhat similar to other generative models)\n\\[\nx_{t-1} = \\frac{1}{\\alpha_t} (x_t - \\beta_t \\epsilon_{t})\n\\]\n\\(x_T\\) is equivalent to a noise and can be sampled from a normal distribution. Therefore, if \\(\\epsilon\\) can be predicted from \\(x_t\\), the reverse step can be repeated a lot of times to generate the data \\(x_0\\) from the sampled noise \\(x_T\\). Here, neural network model comes in to approximate the noise \\(\\epsilon\\) given \\(x_t\\).\n\\[\n\\epsilon = NeuralNetwork(x_t, t)\n\\]\n\n\n\nDetails (Diffusion process)\n\nAlthough the diffusion process is shown in a linear form above, it doesn’t have to be. The more general (or right) way of describing the diffusion process is with a Markov diffusion kernel \\(K\\) [1]:\n\\[\nq(x_t | x_{t-1}) = K_\\pi(x_t | x_{t-1}; \\beta_t^2) \\\\\n\\] \\[\n\\beta^2 := (Diffusion \\ \\ rate) \\\\\n\\] \\[\nq(x_0) := (Data \\ \\ distribution) \\\\\n\\] \\[\n\\pi \\simeq q(x_T) := (Analyticall \\ \\ tractable \\ \\ simple  \\ \\ distribution) \\\\\n\\]\nFor continuous (or almost continous) data, Gaussian distribution is usually chosen as a simple distribution because of its good properties. One of them in case of a diffusion process is that the forward and reverse diffusion kernel become the same gaussian form when the diffusion rate \\(\\beta_t\\) is chosen small.\n\\[\nq(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t^2}x_{t-1}, \\beta_t^2I) \\\\\n\\] \\[\np(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; f_\\mu(x_t, t), f_\\Sigma(x_t, t)) \\\\\n\\] \\[\nf_s := (Approximate \\ \\  function \\ \\  of \\ \\ s)\n\\]\n\n\n\nCode (Diffusion process)\n# construct DDPM linear noise schedule (alphas, betas)\ntimesteps = 1000\nbeta_sqr_1 = 1e-4\nbeta_sqr_T = 0.02\n\nbetas_sqr_t = torch.linspace(beta_sqr_1, beta_sqr_T, timesteps)\n# alphas_sqr_t = 1 - betas_sqr_t\n\n# alphas_sqr_bar_t = torch.cumsum(alphas_sqr_t.log(), dim=0).exp()\n\nbetas_t = betas_sqr_t.sqrt()\n# alphas_t = alphas_sqr_t.sqrt()\n# alphas_bar_t = alphas_sqr_bar_t.sqrt()\n\n\ndef forward_step(x_t, beta, noise=None):\n    \"\"\"Apply a forward diffusion step to `x_t` with `beta` and `noise`\"\"\"\n    if noise is None:\n        noise = torch.randn_like(x_t)\n    alpha = (1 - beta ** 2).sqrt()\n    return alpha * x_t + beta * noise\n\ndef reverse_step(x_t, beta, noise):\n    \"\"\"Apply a reverse diffusion step to `x_t` with `beta` and `noise`\"\"\"\n    alpha = (1 - beta ** 2).sqrt()\n    return (x_t - beta * noise) / alpha\n\ndef forward_t_steps(x_0, t, betas, return_series=False):\n    \"\"\"Apply a forward diffusion step `t` times to `x_0` with `betas`\n    \n    Args:\n        x_0: Initial data\n        t: The number of forward diffusion steps\n        betas: betas[0] ~ betas[t-1] are used for each forward step\n        return_series (bool):\n            If `True`, return all intermediate xs and noises during the diffusion\n            If `False, return the final diffused data\n            \n    Returns:\n        (t+1 diffused data, t added noises) OR\n        final diffused data\n    \"\"\"\n    xs = [x_0]\n    noises = []\n    \n    x_t = x_0\n    for i in range(t):\n        noise = torch.randn_like(x_t)\n        x_t = forward_step(x_t, betas[i], noise)\n        if return_series:\n            noises.append(noise)\n            xs.append(x_t)\n        \n    out = (xs, noises) if return_series else x_t\n    return out\n\ndef reverse_t_steps(x_T, t, betas, noises, return_series=False):\n    \"\"\"Apply a reverse diffusion step `t` times to `x_T` with `betas` and `noises`\n    \n    Args:\n        x_T: Initial noise or diffused data\n        t: The number of reverse diffusion steps\n        betas: betas[t-1] ~ betas[0] are used for each reverse step\n        noises: noises[t-1] ~ noises[0] are eliminated from data in each step\n        return_series (bool):\n            if `True`, return all intermediate xs\n            if `False, return the final denoised data\n        \n    Returns:\n        t+1 denoised data OR final denoised data\n    \"\"\"\n    xs = [x_T]\n    \n    x_t = x_T\n    for i in range(t-1, -1, -1):\n        x_t = reverse_step(x_t, betas[i], noises[i])\n        if return_series:\n            xs.append(x_t)\n        \n    out = xs if return_series else x_t\n    return out\n\n\n\n\n\nxs, epsilons = forward_t_steps(tfm(samp1), 1000, betas_t, True)\nshow_images(xs[::200], 1, 6, titles=title_lists[0], suptitle=suptitles[0])\n\n\n\n\n\n\n\n\ndenoised_xs = reverse_t_steps(xs[-1], 1000, betas_t, epsilons, True)\nshow_images(denoised_xs[::200], 1, 6, titles=title_lists[1], suptitle=suptitles[1])"
  },
  {
    "objectID": "posts/diffusion-model/Untitled.html#neural-network-model",
    "href": "posts/diffusion-model/Untitled.html#neural-network-model",
    "title": "Diffusion based Generative Model",
    "section": "Neural network model",
    "text": "Neural network model\nAs mentioned above, the noise \\(\\epsilon\\) is approximately predicted by the neural network. One advantage of the diffusion model is that it has a freedom to choose any neural network architecture as long as its output \\(\\epsilon\\) is of the same size as its input \\(x\\).\nWith the architecture freedom, the typical choice is the U-net architecture because the skip-connection from the downsampling path (data encoder) to the upsampling path (data decoder) is proven to work well on a similar task.\n\n\n\nUnet architecture (Source)"
  },
  {
    "objectID": "posts/diffusion-model/Untitled.html#noise-scheduler",
    "href": "posts/diffusion-model/Untitled.html#noise-scheduler",
    "title": "Diffusion based Generative Model",
    "section": "Noise scheduler",
    "text": "Noise scheduler\nThe noise scheduler is defined with the parameters of the diffusion process. It is used to generate a corrupted data sample \\(x_t\\) from its clean version \\(x_0\\) for NN model training, and to generate a data sample \\(x_0\\) from noise \\(\\epsilon\\) by applying the reverse-diffusion (denoise) process with the trained NN model.\n\n\\(T\\): the maximum number of steps of the diffusion process\n\\(\\beta_t\\): The strength of a noise \\(\\epsilon\\) added in each diffusion step\n(\\(\\alpha_t\\) can be usually derived from \\(\\beta_t\\))\n\nAs long as \\(\\alpha_t < 1\\) is satisfied, the diffusion process will transform the data \\(x\\) to the normally distributed noise \\(\\epsilon\\). However, it usually has the following properties to make it easier for a neural network to predict the noise of a single diffusion step.\n\n\\(\\alpha_t\\) is a decreasing series from ~1 to ~0\n\\(\\prod_{i=1}^{t}\\alpha_i = \\alpha_1 \\times \\alpha_2 \\times \\cdots \\times \\alpha_t = \\bar{\\alpha}_t\\) shows a linear drop around \\(t \\approx \\frac{T}{2}\\) and an almost flat line with a very little decrease around \\(t \\approx 0\\) and \\(t \\approx T\\)\n\n\n\nDetails (Noise scheduler)\n\nTo be precise, noise scheduler is defined with the transformation of foward/reverse diffusion steps. With the defined diffusion process and given parameters, it should be able to transform data to a noise and denoise the noise back to data.\n\n\n\nCode (U-net)\nclass UnetModel(nn.Module):\n    \"\"\"U-net model\"\"\"\n    \n    def __init__(self, in_channels, out_channels, block_out_channels, max_timestep, \n                 norm_groups=None, norm_eps=1e-6, \n                 block_num_layers=2, temb_channels=None, \n                 emb_type='shift', act_fn=nn.ReLU(), num_classes=None):\n        \"\"\"\n        Args:\n            in_channels: Number of input channels\n            out_channels: Number of output channels\n            block_out_channels: Number of output channels of each UnetDownBlock\n            max_timestep: Maximum timestep (NoiseScheduler.max_t)\n            norm_groups: Number of channel groups used in GroupNorm\n            norm_eps: Value for stability in GroupNorm\n            block_num_layers: Number of Residual blocks to be used in downsampling\n                              (block_num_layers + 1 Residual blocks in upsampling)\n            temb_channels: Number of timestep embedding channels\n                           If None, default to 4 *  block_out_channels[0]\n            emb_type: 'shift': additive, 'scale_shift': additive + multiplicative\n                If 'shift', h -> h + emb\n                If 'scale_shift', h -> h * (1 + emb[:n/2]) + emb[n/2:]\n            act_fn: Activation function to be used\n            num_classes: If not None, number of possible conditions\n        \"\"\"\n        super().__init__()\n\n        if not temb_channels: # Timestep embedding channels\n            temb_channels = 4 * block_out_channels[0]\n        if not norm_groups:\n            norm_groups = block_out_channels[0] // 4\n        if len(block_out_channels) > 0:\n            block_out_channels += [block_out_channels[-1]]\n\n        # Class condition embedding\n        cemb_channels = 0\n        self.cemb_module = None\n        if num_classes is not None:\n            cemb_channels = temb_channels//4\n            self.cemb_module = nn.Embedding(num_classes, cemb_channels)\n\n        # Timestep embedding module\n        self.temb_module = \\\n            TimestepEmbedding(temb_channels//2, temb_channels, \n                              act_fn=act_fn, max_timestep=max_timestep)\n\n        # First convolution\n        self.conv_in = nn.Sequential(\n            nn.Conv2d(in_channels, block_out_channels[0], 3, stride=1, padding=1)\n        )\n        \n        # Downsample and Upsample blocks\n        self.down_blocks = nn.ModuleList()\n        self.up_blocks = nn.ModuleList()\n\n        # Last convolution\n        self.conv_out = nn.Sequential(\n            nn.GroupNorm(norm_groups, block_out_channels[0], norm_eps, affine=True),\n            act_fn,\n            nn.Conv2d(block_out_channels[0], out_channels, 3, stride=1, padding=1)\n        )\n\n        # Downsample blocks\n        out_channels = block_out_channels[0]\n        for i in range(len(block_out_channels)):\n            downsample = False if i == len(block_out_channels) - 1 else True\n            \n            in_channels = out_channels\n            out_channels = block_out_channels[i]\n\n            self.down_blocks.append(\n                UnetDownBlock(\n                    in_channels,\n                    out_channels,\n                    temb_channels + cemb_channels,\n                    norm_groups,\n                    norm_eps,\n                    num_layers=block_num_layers,\n                    emb_type=emb_type,\n                    act_fn=act_fn,\n                    downsample=downsample\n                )\n            )\n\n        # Upsample blocks\n        reversed_block_out_channels = list(reversed(block_out_channels))\n        out_channels = reversed_block_out_channels[0]\n        for i in range(len(reversed_block_out_channels)):\n            upsample = False if i == len(reversed_block_out_channels) - 1 else True\n            \n            in_channels = out_channels\n            out_channels = reversed_block_out_channels[i]\n            down_channel_idx = min(i + 1, len(reversed_block_out_channels) - 1)\n            down_block_in_channels = reversed_block_out_channels[down_channel_idx]\n\n            self.up_blocks.append(\n                UnetUpBlock(\n                    in_channels,\n                    out_channels,\n                    down_block_in_channels,\n                    temb_channels + cemb_channels,\n                    norm_groups,\n                    norm_eps,\n                    num_layers=block_num_layers + 1,\n                    emb_type=emb_type,\n                    act_fn=act_fn,\n                    upsample=upsample\n                )\n            )\n\n    def forward(self, x, t, c=None):\n        \"\"\"\n        Args:\n            x: Input data\n            t: Input timestep\n            c: Optional condition (class) input\n        \"\"\"\n        # Create an embedding of timestep (+ class)\n        emb = self.temb_module(t)\n        if self.cemb_module is not None:\n            cemb = self.cemb_module(c)\n            emb = torch.cat((emb, cemb), dim=1)\n\n        # Input convolution\n        h = self.conv_in(x)\n\n        # Downsampling path\n        h_all_states = (h, )\n        for down_block in self.down_blocks:\n            h, h_states = down_block(h, emb)\n\n            h_all_states += h_states\n            \n        # Upsampling path\n        for up_block in self.up_blocks:\n            # Skip-connections\n            skip_h_states = h_all_states[-up_block.num_layers:]\n            h_all_states = h_all_states[:-up_block.num_layers]\n\n            h = up_block(h, skip_h_states, emb)\n\n        out = self.conv_out(h)\n\n        return out\n\n    \nclass TimestepEmbedding(nn.Module):\n    \"\"\"Timestep embedding module\"\"\"\n    \n    def __init__(self, hidden_dim, embedding_dim, act_fn, \n                 max_timestep=1e3, scale=math.pi/2):\n        \"\"\"\n        Args:\n            hidden_dim: Dimension of intermediate hidden layer\n            embedding_dim: Embedding dimension\n            act_fn: Activation function to be used\n            max_timestep: Maximum timestep (NoiseScheduler.max_t)\n        \"\"\"\n        super().__init__()\n        \n        self.h_dim = hidden_dim #embedding_dim // 4\n        self.emb_dim = embedding_dim\n        self.max_t = max_timestep\n        self.scale = scale\n        \n        self.layers = nn.Sequential(*[\n            nn.Linear(self.h_dim, self.emb_dim),\n            act_fn,\n            nn.Linear(self.emb_dim, self.emb_dim)\n        ])\n\n    def forward(self, timesteps):\n        \"\"\"\n        Args:\n            timesteps: Batch of timesteps\n            \n        Returns:\n            Batch of timestep embeddings\n        \"\"\"\n        # Create const-like, linear-like and random-like embeddings\n        half_h_dim = self.h_dim // 2\n        exponent = -math.log(10 * self.max_t) / half_h_dim * \\\n            torch.arange(0, half_h_dim, device=timesteps.device)\n        emb = torch.exp(exponent) * self.scale\n        emb = timesteps[:, None].float() * emb[None, :]\n\n        emb = torch.cat([torch.cos(emb), torch.sin(emb)], dim=-1)\n        \n        temb = self.layers(emb)\n        \n        return temb\n\n    \nclass UnetDownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, emb_channels, norm_groups,\n                 norm_eps=1e-6, num_layers=2, emb_type='shift', \n                 act_fn=nn.ReLU(), downsample=True, output_scale=1.0):\n        \"\"\"\n        Args:\n            in_channels: Number of input channels\n            out_channels: Number of output channels\n            emb_channels: Number of embedding channels\n                          (timestep or timestep + condition embedding)\n            norm_groups: Number of channel groups used in GroupNorm\n            norm_eps: Value for stability in GroupNorm\n            num_layers: Number of Residual blocks to be used\n            emb_type: 'shift': additive, 'scale_shift': additive + multiplicative\n                If 'shift', h -> h + emb\n                If 'scale_shift', h -> h * (1 + emb[:n/2]) + emb[n/2:]\n            act_fn: Activation function to be used\n            downsample: whether or not to apply downsampling \n        \"\"\"\n        super().__init__()\n\n        self.res_blocks = nn.ModuleList()\n        self.downsample_blocks = nn.ModuleList()\n\n        # Residual blocks\n        for i in range(num_layers):\n            in_ch = in_channels\n            out_ch = in_channels\n\n            self.res_blocks.append(\n                ResBlock(in_ch, out_ch, emb_channels, norm_groups, norm_eps, \n                         emb_type, act_fn, init_zero=True, \n                         output_scale=output_scale)\n            )\n        \n        # Downsampling modules\n        if downsample:\n            self.downsample_blocks.append(\n                DownSample2D(in_channels, out_channels, \n                             norm_groups, norm_eps, act_fn, output_scale)\n            )\n\n    def forward(self, x, emb):\n        # Gather hidden states for skip-connections\n        h_states = ()\n\n        h = x\n        for block in self.res_blocks:\n            h = block(h, emb)\n            h_states += (h, )\n        \n        for block in self.downsample_blocks:\n            h = block(h)\n            h_states += (h, )\n\n        return h, h_states\n\n\nclass UnetUpBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, down_in_channels, emb_channels, \n                 norm_groups, norm_eps=1e-6, num_layers=3, emb_type='shift', \n                 act_fn=nn.ReLU(), upsample=True, output_scale=1.0):\n        \"\"\"\n        Args:\n            in_channels: Number of input channels\n            out_channels: Number of output channels\n            down_in_channels: Number of skip-connection channels\n            emb_channels: Number of embedding channels\n                          (timestep or timestep + condition embedding)\n            norm_groups: Number of channel groups used in GroupNorm\n            norm_eps: Value for stability in GroupNorm\n            num_layers: Number of Residual blocks to be used\n            emb_type: 'shift': additive, 'scale_shift': additive + multiplicative\n                If 'shift', h -> h + emb\n                If 'scale_shift', h -> h * (1 + emb[:n/2]) + emb[n/2:]\n            act_fn: Activation function to be used\n            upsample: whether or not to apply downsampling \n        \"\"\"\n        super().__init__()\n        \n        self.num_layers = num_layers\n\n        self.res_blocks = nn.ModuleList()\n        self.upsample_blocks = nn.ModuleList()\n        \n        # Residual blocks\n        for i in range(num_layers):\n            skip_channels = down_in_channels\n            in_ch = (out_channels) + skip_channels\n            out_ch = out_channels \n            \n            self.res_blocks.append(\n                ResBlock(in_ch, out_ch, emb_channels, norm_groups, norm_eps, \n                         emb_type, act_fn, output_scale=output_scale)\n            )\n        \n        # Upsampling modules\n        if upsample:\n            self.upsample_blocks.append(Upsample2D(out_ch, down_in_channels))\n\n\n    def forward(self, x, skip_states, emb):\n        h = x\n        for block in self.res_blocks:\n            # skip connection from the downsampling path of Unet\n            skip_h = skip_states[-1]\n            skip_states = skip_states[:-1]\n            h = torch.cat((h, skip_h), dim=1)\n\n            h = block(h, emb)\n\n        for block in self.upsample_blocks:\n            h = block(h)\n        \n        return h\n\n    \nclass DownSample2D(nn.Module):\n    \"\"\"2D downsampling module with residual-like layers\"\"\"\n    \n    def __init__(self, in_channels, out_channels, norm_groups,\n                 norm_eps=1e-6, act_fn=nn.ReLU(), output_scale=1.0):\n        \"\"\"\n        Args:\n            in_channels: Number of input channels\n            out_channels: Number of output channels\n            norm_groups: Number of channel groups used in GroupNorm\n            norm_eps: Value for stability in GroupNorm\n            act_fn: Activation function to be used\n        \"\"\"\n        super().__init__()\n        \n        self.output_scale = output_scale\n        self.act_fn = act_fn\n\n        # 1st path of 3x3 convolutions\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, \n                               stride=2, padding=1, bias=False)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, \n                               stride=1, padding=1, bias=False)\n        \n        # 2nd path of polling and 1x1 convolution\n        self.pool  = nn.AvgPool2d(2, 2)\n        self.conv3 = nn.Conv2d(in_channels, out_channels, 1, 1, bias=False)\n        \n        self.norm1 = nn.GroupNorm(norm_groups, in_channels, norm_eps, affine=True)\n        self.norm2 = nn.GroupNorm(norm_groups, out_channels, norm_eps, affine=True)\n        \n        # Init residual path to zero\n        nn.init.zeros_(self.conv2.weight)\n        \n    def forward(self, x):\n        x = self.act_fn(self.norm1(x))\n        \n        # 1st path\n        h1 = self.conv1(x)\n        h1 = self.act_fn(self.norm2(h1))\n        h1 = self.conv2(h1)\n        \n        # 2nd path\n        h2 = self.conv3(self.pool(x))\n        \n        return (h1 + h2) / self.output_scale\n\n\nclass Upsample2D(nn.Module):\n    \"\"\"2D upsampling module\"\"\"\n    \n    def __init__(self, in_channels, out_channels, \n                 scale=2.0, interpolate_mode='bilinear'):\n        \"\"\"\n        Args:\n            in_channels: Number of input channels\n            out_channels: Number of output channels\n            scale: Upsampling ratio\n            interpolation_mode: Upsampling interpolation mode\n        \"\"\"\n        super().__init__()\n        \n        self.scale = scale\n        self.interpolate_mode = interpolate_mode\n        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n\n    def forward(self, x):\n        h = F.interpolate(x, scale_factor=self.scale, mode=self.interpolate_mode)\n        h = self.conv(h)\n        return h\n\n    \nclass ResBlock(nn.Module):\n    \"\"\"Residual block\"\"\"\n    \n    def __init__(self, in_channels, out_channels, emb_channels, norm_groups, \n                 norm_eps=1e-6, emb_type='shift', act_fn=nn.ReLU(),\n                 skip_time_act=False, init_zero=False, output_scale=1.0):\n        \"\"\"\n        Args:\n            in_channels: Number of input channels\n            out_channels: Number of output channels\n            emb_channels: Number of embedding channels\n                          (timestep or timestep + condition embedding)\n            norm_groups: Number of channel groups used in GroupNorm\n            norm_eps: Value for stability in GroupNorm\n            emb_type: 'shift': additive, 'scale_shift': additive + multiplicative\n                If 'shift', h -> h + emb\n                If 'scale_shift', h -> h * (1 + emb[:n/2]) + emb[n/2:]\n            act_fn: Activation function to be used\n            skip_time_act: whether or not to skip pre-activation of input embedding\n            init_zero: whether or not to init last module of conv path to zero\n        \"\"\"\n        super().__init__()\n\n        self.emb_type = emb_type\n        self.act_fn = act_fn\n        self.skip_time_act = skip_time_act\n        self.output_scale = output_scale\n\n        # Convolution path\n        self.norm1 = nn.GroupNorm(norm_groups, in_channels, norm_eps, affine=True)\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, \n                               stride=1, padding=1, bias=False)\n        self.norm2 = nn.GroupNorm(norm_groups, out_channels, norm_eps, affine=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, \n                               stride=1, padding=1, bias=False)\n\n        if emb_type == 'scale_shift':\n            self.emb_proj = nn.Linear(emb_channels, 2*out_channels)\n        else:\n            self.emb_proj = nn.Linear(emb_channels, out_channels)\n            \n        # Residual path\n        self.conv3 = nn.Identity()\n        if in_channels != out_channels:\n            self.conv3 = nn.Conv2d(in_channels, out_channels, 1, \n                                   stride=1, padding=0, bias=True)\n            \n        if init_zero:\n            nn.init.zeros_(self.conv2.weight)\n\n    def forward(self, x, emb):\n        # Process input embedding into offset (or scale + offset)\n        if self.emb_type is not None:\n            if not self.skip_time_act:\n                emb = self.act_fn(emb)\n            emb = self.emb_proj(emb)[:, :, None, None] # unsqueeze over H x W\n\n            scale = 0\n            offset = emb\n            if self.emb_type == 'scale_shift':\n                scale, offset = torch.chunk(emb, 2, dim=1)\n\n        # Convolution path\n        h = self.act_fn(self.norm1(x))\n        h = self.conv1(h)\n\n        h = h * (1 + scale) + offset\n\n        h = self.act_fn(self.norm2(h))\n        h = self.conv2(h)\n\n        return (self.conv3(x) + h) / self.output_scale\n\n\n\n\nCode (Noise scheduler)\nclass NoiseScheduler:\n    \"\"\"Implementation of forward and reverse diffusion processes\n    \n    Parameters:\n        max_t: maximum possible timestep T\n        betas: A series of stdev of normally distributed noises\n        ts: A series of increasing integers 1 ~ T\n    \n    Methods:\n        add_noise: Apply the forward diffusion step t times\n        denoise: Apply a single reverse diffusion step\n    \"\"\"\n    def __init__(self, max_t, betas):\n        self.max_t = max_t\n        self.betas = betas\n        self.ts = list(range(1, max_t + 1))\n        \n    def _process(self, step_func, iterable):\n        processed_xb = []\n        for step_func_args in iterable:\n            x_t = step_func(*step_func_args)\n            processed_xb.append(x_t)\n        processed_xb = torch.stack(processed_xb)\n        \n        return processed_xb\n    \n    def to(self, device):\n        self.betas = self.betas.to(device)\n        return self\n    \n    def add_noise(self, xb, tb, noiseb):\n        \"\"\"Apply the forward diffusion step `tb` times to `xb` with `noiseb`\n        \n        Args:\n            xb: Batch of data x\n            tb: Batch of timesteps t\n            noiseb: Batch of noises that will be added to `xb`\n        \n        Returns:\n            Batch of diffused data\n        \"\"\"\n        if isinstance(xb, np.ndarray):\n            xb = batch_tfm(xb)\n            \n        if noiseb is None:\n            betasb = (self.betas[:t] for t in tb)\n            diffused_xb = self._process(forward_t_steps, \n                                        zip(xb, betasb))\n        else:\n            diffused_xb = self._process(forward_step, \n                                        zip(xb, self.betas[tb-1], noiseb))\n        return diffused_xb\n    \n    def denoise(self, xb, t, noiseb):\n        \"\"\"Apply a single reverse diffusion step to `xb` with `noiseb` at `t`\n        \n        Args:\n            xb: Batch of diffused data x\n            t: Current timestep of `xb`. This controls the amount of noise to remove.\n            noiseb: Batch of noises to eliminate from `xb`\n        \n        Returns:\n            Batch of data denoised by a single reverse step\n        \"\"\"\n        if isinstance(xb, np.ndarray):\n            xb = batch_tfm(xb)\n        iterable = zip(xb, self.betas[[t-1] * len(xb)], noiseb)\n        denoised_xb = self._process(reverse_step, iterable)\n        return denoised_xb\n\n\n\n\nCode (Diffusion model)\nclass DiffusionModelBase:\n    \"\"\"Diffusion model that can train a noise predictor and generate data\n    \n    Parameters:\n        noise_scheduler (NoiseScheduler): Diffusion process implementation\n        noise_predictor (torch.nn.Module): Noise predicting model\n        optim (torch.optim.Optimizer): Optimiser of `noise_predictor` \n    \n    Methods:\n        train: Train `noise_predictor` with diffused data samples\n        generate: Generate denoised clean data from normally distributed noise\n    \"\"\"\n    def __init__(self, nn_model, noise_scheduler, optimizer=None, device='cuda'):\n        self.device = device\n        self.noise_predictor = nn_model.to(device)\n        self.noise_scheduler = noise_scheduler.to(device)\n        if optimizer is None:\n            self.optim = torch.optim.Adam(self.noise_predictor.parameters(), lr=1e-3)\n        else:\n            self.optim = optimizer\n            \n        \n    def _diffuse_data(self, xb, tb, noiseb):\n        # Add `noiseb` to (`tb` - 1)th diffused data \n        diffused_xb_pre = self.noise_scheduler.add_noise(xb, tb - 1, None)\n        diffused_xb = self.noise_scheduler.add_noise(diffused_xb_pre, tb, noiseb)\n        return diffused_xb\n    \n    def _predict_noise(self, xb, tb, cb):\n        return self.noise_predictor(xb, tb)\n    \n    def train(self, dataloader, n_epoch=5):\n        \"\"\"Train `noise_preditor` on `dataloader`\n        \n        Args:\n            dataloader (torch.utils.data.DataLoader): Iterable training data\n            n_epoch: Number of epochs to train `noise_predictor`\n        \"\"\"\n        max_t = self.noise_scheduler.max_t\n        base_lr = self.optim.param_groups[0]['lr']\n        losses = []\n\n        self.noise_predictor.train()\n#         step = 0\n\n        for ep in range(n_epoch):\n            # Exponential decay learning rate per epoch\n            self.optim.param_groups[0]['lr'] = base_lr/(2**ep)\n\n            extra_info = {'epoch': ep}\n            dl = pbar(dataloader, extra_info, min_interval=2)\n            for xb, yb in dl:   # x: images, y: labels\n#                 self.optim.param_groups[0]['lr'] = base_lr * lr_lambda(step)\n#                 step += 1\n                self.optim.zero_grad()\n                xb = xb.to(self.device)\n\n                # Perturb data\n                noiseb = torch.randn_like(xb)\n                tb = torch.randint(1, max_t + 1, (xb.shape[0],)).to(self.device)\n                \n                diffused_xb = self._diffuse_data(xb, tb, noiseb)\n\n                # Use neural network to predict noise\n                pred_noiseb = self._predict_noise(diffused_xb, tb, yb)\n\n                # Loss between the predicted and true noise\n                loss = F.mse_loss(pred_noiseb, noiseb)\n                loss.backward()\n                self.optim.step()\n                \n                # Show learning-rate and 100-step mean of loss with a progress bar\n                losses.append(loss.item())\n                extra_info['loss'] = np.mean(losses[-100:]).round(4)\n                extra_info['lr'] = self.optim.param_groups[0]['lr']\n\n    @torch.no_grad()\n    def generate(self, n_sample, n_ch, sz, \n                 cb=None, seed=None, return_intermediates=False):\n        \"\"\"Generate data (`n_sample` x `n_ch` x `sz` x `sz`)\n        \n        Args:\n            n_sample: Number of data samples to generate\n            n_ch: Number of channel\n            sz: Height and width\n            cb: Optional batch of condition c\n                (It requires `noise_predictor` to be a conditional model)\n            seed: Optional random seed\n            return_intermediates (bool):\n                if `True`, return all intermediate data during generation process\n                if `False, return the final generated data\n            \n        Returns:\n            (final generated data, (intermediate timesteps, intermediate data)) OR\n            final generated data\n        \"\"\"\n        max_t = self.noise_scheduler.max_t\n        samples = torch.randn(n_sample, n_ch, sz, sz, generator=seed)\n        samples = samples.to(self.device)\n        intermediate_imgs = [samples.detach().cpu()]\n        intermediate_ts = [max_t]\n        \n        self.noise_predictor.eval()\n        \n        extra_info = {}\n        reversed_ts = self.noise_scheduler.ts[::-1]\n        progress = pbar(reversed_ts, extra_info)\n        for i, t in enumerate(progress):\n            tb = torch.tensor([t]*n_sample).to(self.device)\n\n            # Use the trained noise_predictor to predict noise\n            pred_noiseb = self._predict_noise(samples, tb, cb)\n            \n            # Apply a reverse diffusion step (denoise step)\n            samples = self.noise_scheduler.denoise(samples, t, pred_noiseb)\n            \n            # Sample intermiedate data with rate of (max_t // 100)\n            if return_intermediates:\n                if i % (max_t // 100) == 0 or t < 5:\n                    intermediate_imgs.append(samples.detach().cpu())\n                    intermediate_ts.append(t)\n\n        intermediates = (intermediate_ts, torch.stack(intermediate_imgs))\n        return (samples, intermediates) if return_intermediates else samples\n\n    \n# def lr_lambda(current_step):\n#     num_warmup_steps = 350\n#     num_training_steps = 3744\n#     num_cycles = 0.5\n#     if current_step < num_warmup_steps:\n#         return float(current_step) / float(max(1, num_warmup_steps))\n#     progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n#     return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))"
  },
  {
    "objectID": "posts/diffusion-model/Untitled.html#conditional-model-and-guidance-technique",
    "href": "posts/diffusion-model/Untitled.html#conditional-model-and-guidance-technique",
    "title": "Diffusion based Generative Model",
    "section": "Conditional model and Guidance technique",
    "text": "Conditional model and Guidance technique\n\n\n\n\n\nIf there is a mechanism to control the generated data, it will be more useful. One typical method in deep learning is to add additional input data \\(c\\) such that the model generates data relevant to the information \\(c\\) from a random noise \\(z\\).\nAnother common method in a diffusion model is called Guidance. The reverse diffusion step can be viewed as a denoising step: Given a noise-like data \\(x_t\\), find out less noise-like one \\(x_{t-1}\\) by predicting the added noise \\(\\epsilon_t\\) and eliminating it from \\(x_t\\). Guidance changes the noise prediction \\(\\epsilon\\) with the condition information \\(c\\) such that \\(x_{t-1}\\) is closer to the data \\(x\\) relevant to \\(c\\) out of all other possible \\(x\\). Parameter called Guidance scale determines the strength towards the controlled generation.\n\n\n\n\n\nThere are 2 kinds of guidance tecniques:\n\nClassifier-based guidance: It uses a seperately trained classifier model that can predict \\(p(c|x)\\). As it’s trained with \\(x\\) and \\(c\\), it knows about the relationship between \\(x\\) and \\(c\\), and by updating \\(\\epsilon\\) with this knowledge in a specific math formula, it can control the data generation process with \\(c\\) [4].\nClassifier-free guidance: It uses a single conditional diffusion model that takes \\(x\\) and \\(c\\) as inputs. One requirement, unlike the typical conditional model, is that the model should be able to predict unconditionally as well when \\(c\\) is not given. In each reverse diffusion step, the neural network model will predict \\(\\epsilon\\) twice, conditioned on \\(c\\) and unconditionally, the difference between these 2 prediction is used to update \\(\\epsilon\\) [5].\n\nThere could be a question for a classifier-free guidance like “Why is it needed when the conditional model can already generate a controlled data with the condition information \\(c\\)?” It’s true that this technique is not mandatory when a conditional model is trained. But it’s shown that the guidance can additionally improve the generated data quality while sacrificing sample diversity a bit.\n\n\nDetails (Mathematical formulation of guidance technique)\n\nWhen the probabilty follows a normal distribution, its score function is related to a noise as follows: \\[\n\\nabla_{x} log \\ p(x) = \\nabla_{x} (- \\frac{(x-\\mu)^2}{2 \\ \\sigma^2}) = - \\frac{\\epsilon}{\\sigma} \\ \\ \\ \\ \\ (\\epsilon \\sim \\mathcal{N}(0, 1))\n\\]\nUsing this relationship, the classifier-based guidance modifies the noise prediction by matching \\(\\nabla_{x_t} log \\ p_{\\theta, \\phi}(x_t | c)\\) to \\(\\nabla_{x_t} log \\ p_\\theta(x_t)\\) with a separate classifier \\(p_\\phi(c|x)\\).\n\\[\n\\begin{align}\n\\nabla_{x_t} log \\ p_{\\theta, \\phi}(x_t | c) &= \\nabla_{x_t} log \\ p_\\theta(x_t) + \\nabla_{x_t} log \\ p_\\phi(c | x_t) \\ \\ \\ \\ \\ \\because Bayes' \\ theorem \\\\\n    &\\simeq -\\frac{1}{\\sigma} (\\bar{\\epsilon}_t - \\sigma \\cdot \\nabla_{x_t} log \\ p_\\phi(c|x_t)) \\\\\n\\therefore \\tilde{\\epsilon}_t &= \\bar{\\epsilon}_t - s \\cdot \\sigma \\cdot \\nabla_{x_t} log \\ p_\\phi(c|x_t))\n\\end{align}\n\\]\nThe classifier-free guidance uses a single conditional model to model the classifier score function \\(\\nabla log \\ p(c|x)\\). \\[\n\\begin{align}\n\\nabla_{x_t} log \\ p(c|x_t) &= \\nabla_{x_t} log \\ p(x_t|c) - \\nabla_{x_t} log \\ p(x_t) \\ \\ \\ \\ \\ \\because Bayes' \\ theorem \\\\\n\\therefore \\tilde{\\epsilon}_t &= \\bar{\\epsilon}_t(x_t) - s \\cdot \\sigma \\cdot - \\frac{1}{\\sigma} (\\bar{\\epsilon}_t(x_t, c) - \\bar{\\epsilon}_t(x_t)) \\\\\n    &= \\bar{\\epsilon}_t(x_t) + s \\cdot (\\bar{\\epsilon}_t(x_t, c) - \\bar{\\epsilon}_t(x_t))\n\\end{align}\n\\]\n\n\n\nCode (Conditional model and Guidance)\nclass ConditionalDiffusionModel(DiffusionModel):\n    \"\"\"Conditional diffusion model that can train a noise predictor and \n       generate data conditionally\n    \n    Parameters:\n        noise_scheduler (NoiseScheduler): Diffusion process implementation\n        noise_predictor (torch.nn.Model): Noise predicting model\n        optim (torch.optim.Optimizer): Optimiser of `noise_predictor` \n        uncond_label: Integer (>= 0) label to indicate an unconditional input\n                      (20% of input labels(conditions) will be randomly set)\n        g_scale: Optional guidance scale \n                 (if not None, guidance technique is used)\n    \n    Methods:\n        train: Train `noise_predictor` with diffused data samples\n        generate: Generate denoised clean data from normally distributed noise\n    \"\"\"\n    \n    def __init__(self, uncond_label, guidance_scale=None, **kwargs):\n        super().__init__(**kwargs)\n        self.uncond_label = uncond_label\n        self.g_scale = guidance_scale \n    \n    def _predict_noise(self, xb, tb, cb):\n        # Move batch of conditional information to `self.device`\n        cb = cb.to(self.device)\n        \n        if self.noise_predictor.training:\n            # randomly set 20% of condition labels to uncond_label \n            # to train a nn_model to make an unconditional inference\n            idxs = np.random.choice(range(len(cb)), len(cb)//5, replace=False)\n            cb[idxs] = self.uncond_label\n            \n            noise = self.noise_predictor(xb, tb, cb)\n        else:\n            # Predict noise twice: conditional and unconditional inferences\n            if self.g_scale is not None:\n                xb = xb.repeat(2, 1, 1, 1)\n                tb = tb.repeat(2)\n                cb = torch.cat((cb, torch.full_like(cb, self.uncond_label)))\n                \n            noise = self.noise_predictor(xb, tb, cb)\n            \n            if self.g_scale is not None:\n                # Update noise with classifier-free guidance\n                bs = len(noise)//2\n                guidance = noise[:bs] - noise[bs:]\n                noise = noise[bs:] + self.g_scale * guidance\n                \n        return noise\n    \n\n# Conditional diffusion model training on FMNIST dataset\nn_labels = len(dataset.y_name) # Number of labels (conditions)\n\nnn_model = UnetModel(\n    in_channels=1,\n    out_channels=1,\n    block_num_layers=2,\n    block_out_channels=[32, 64, 128],\n    max_timestep=NUM_TIMESTEPS,\n    norm_groups=8,\n    num_classes=(n_labels + 1) # Include unconditional label\n)\n# Init embedding of uncoditional label(n_labels==10) to 0\nnn_model.cemb_module.weight.data[n_labels] = 0\n\n\nCondDDPMModel = ConditionalDiffusionModel(\n    uncond_label = n_labels,\n#     guidance_scale = 3.0,\n    nn_model = nn_model,\n    noise_scheduler = DDPMScheduler\n)\n\nCondDDPMModel.train(dataloader, 5)\n\n\n\n\n\nn_samples = 10\nlabel = 'Ankle boot'\ncondition = dataset.y_name.index(label)\nconds = torch.tensor(condition).repeat_interleave(n_samples)\n\nCondDDPMModel.g_scale = None\ncond_imgs = CondDDPMModel.generate(n_samples, n_ch=1, sz=32, cb=conds)\nshow_images(cond_imgs, 1, 10, suptitle=suptitles[3].format(label))\n\n\n\n\n\n\n\n\nCondDDPMModel.g_scale = 3.0\ncond_guide_imgs = CondDDPMModel.generate(n_samples, n_ch=1, sz=32, cb=conds)\nshow_images(cond_guide_imgs, 1, 10, suptitle=suptitles[4].format(label))"
  },
  {
    "objectID": "posts/diffusion-model/Untitled.html#nobility-vs-memorisation",
    "href": "posts/diffusion-model/Untitled.html#nobility-vs-memorisation",
    "title": "Diffusion based Generative Model",
    "section": "Nobility vs Memorisation",
    "text": "Nobility vs Memorisation\n\n\n\nGenerated pokemon sprites (blue-box: noble+memorised sample, red-box: memorised sample, others: noble sample)\n\n\nThere could be a question,\n“Okay, the diffusion model seems to be able to generate something that looks real, but isn’t it just one of the data the model has already seen? It doesn’t seem to have any special component that helps to create a noble one. How can it be useful if it just copies and pastes one of the data it has seen during training?”\nIt is true that the diffusion model memorises non-diffused data to do a denoising step effectively. The memorisation of a whole sample happens. However, that’s not the only thing that the model learns. Inductive bias of a neural network plus a sauce of randomness helps learn the common parts/characteristics of multiple samples as well. As a result, the model will not only copy the data but also generate a noble new sample. This seems analogus to how human creativity comes out of knowledge with a few memory errors."
  },
  {
    "objectID": "posts/diffusion-model/diffusion.html",
    "href": "posts/diffusion-model/diffusion.html",
    "title": "Diffusion based Generative Model",
    "section": "",
    "text": "(a) Image AI product (Source: NVIDIA canvas)\n\n\n\n\n\n\n\n(b) Controlled image generation  (Source: ControlNet)\n\n\n\n\nFigure 1: Examples of generative AI\n\n\n\n\n\n\n\nGenerative AI has become popular through the media and it started becoming a real tool in the industry rather than just a research topic. The advance speed of AI is so fast that many companies started creating a product utilising such AI technologies.\nDiffusion model is one of the state-of-art approaches in Generative AI. It is used to generate an image, music, video and any data having an inherent continuity characteristic. Recent image generation products, Midjourney and Dreamstudio, are all based on the diffusion model.\nWhat is special about the diffusion model? What kind of mathematical background is it based on? What kinds of techinques are used in the diffusion model? Let’s go into the details with code examples to answer some of these questions.\n\n\nDetails (Intro)\n\nGenerative models are trying to model the full input data distribution of \\(p(x)\\) whereas discriminative models fit on the conditional label distribution of \\(p(y|x)\\). As a result, it has been more difficult to create a generative model because \\(x\\) that people wanted to generate, was usually a high dimensional unstructured data like image or text that is much more complex than the label distribution of \\(y\\).\nDespite the difficulty of creating the high-quality generative models, There has been many attempts to tackle the generative problem with various deep learning approaches.\n\nVariational AutoEncoder: VAE has 2 components: encoder for \\(p(z|x)\\) and decoder for \\(p(x|z)\\). \\(z\\) is called a latent variable or it can be thought as a special hidden state. The model is trained to generate \\(x\\) from \\(z\\) while enforcing \\(z\\) to be normally distributed.\nGenerative Adversarial Network: GAN also has 2 components: discriminator for \\(p(IsReal|x)\\) and generator for \\(p(x|z)\\). \\(z\\) can be theoretically any distribution that can be sampled from, but normal distribution is usually used. As discriminator’s capability to differentiate ‘real \\(x\\)’ from ‘generated \\(x\\)’ is used to train a generator, it is important to train both components adequately and simultaneously in a way that they can co-help each other during training.\nNormalising flow based model: This is based on an invertible transformation to change a simple distribution to a more complex one. It creates a sequential process where each sequential step transforms a distribution slightly such that a simple distribution like normal distrubtion is ultimately transformed to the data distribution at the end. Then, the model can be directly optimised to maximise \\(p(x)\\).\n\nSo, “Is the diffusion model just another approach?” The answer is Yes, with advantages over the other approaches like being more stable than GAN, more powerful than VAE, not restricted to an invertible transformation like normalising flow models.\n\n\n\nCommand to install dependency libraries\n# !pip3 install torch, numpy, matplotlib\n\n\n\n\nCode (Data)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math, gzip, urllib, enum\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom matplotlib.animation import FuncAnimation, PillowWriter\nfrom IPython.display import HTML\n\n# Sample image data\nsamp1 = b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00000000\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00000000\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00000000P\\xd0\\xf8\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00P\\xd0\\xf8000000\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8\\x00\\x00\\x008Pp8Pp8Pp8Pp\\x00\\x00\\x00P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00@\\xa0\\xc8P\\xd0\\xf8P\\xd0\\xf88p\\x98@\\xa0\\xc8P\\xd0\\xf8P\\xd0\\xf8@\\xa0\\xc88p\\x98P\\xd0\\xf8P\\xd0\\xf8@\\xa0\\xc8\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00@\\xa0\\xc8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff000@\\xa0\\xc8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8@\\xa0\\xc8000\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff000000\\x00\\x00\\x00P\\xd0\\xf8P\\xd0\\xf8\\xf8\\xe8\\xe8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8\\xf8\\xe8\\xe8P\\xd0\\xf8P\\xd0\\xf8\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff0008p\\x98\\x00\\x00\\x008X\\xc8P\\xd0\\xf8\\x00\\x00\\x00P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8P\\xd0\\xf8\\x00\\x00\\x00P\\xd0\\xf88X\\xc8\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff0008p\\x988p\\x98\\x188\\x908X\\xc8P\\xd0\\xf8P\\xd0\\xf88p\\x988p\\x98P\\xd0\\xf8P\\xd0\\xf88X\\xc8\\x188\\x90\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff0008p\\x988p\\x98000\\x00\\x00\\x00000@\\xa0\\xc8@\\xa0\\xc8@\\xa0\\xc8@\\xa0\\xc8000\\x00\\x00\\x00000\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff0008p\\x988Pp@\\xa0\\xc88Pp0000000000008Pp@\\xa0\\xc88Pp8Pp\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff8Pp\\x00\\x00\\x00@\\xa0\\xc8P\\xd0\\xf88Pp@\\xa0\\xc8@\\xa0\\xc88PpP\\xd0\\xf8@\\xa0\\xc8\\x00\\x00\\x008Pp\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff8Pp@\\xa0\\xc8\\x00\\x00\\x00\\x00\\x00\\x00@\\xa0\\xc8@\\xa0\\xc8@\\xa0\\xc8@\\xa0\\xc8\\x00\\x00\\x00\\x00\\x00\\x00@\\xa0\\xc88Pp\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff8Pp@\\xa0\\xc8@\\xa0\\xc8@\\xa0\\xc8@\\xa0\\xc8@\\xa0\\xc8@\\xa0\\xc8@\\xa0\\xc8@\\xa0\\xc8\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x008p\\x98@\\xa0\\xc8000000000000\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff'\nsamp2 = b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18(H\\x18(H\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18(H\\x18(H\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18(H(Hp\\x18(H\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18(H(Hp\\x18(H\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18(H(Hp\\x18(H\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18(H(Hp\\x18(H\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00(Hp(Hp\\x18(H\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18(H(Hp(Hp\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x18\\xb0\\xd0(Hp\\x18(H\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18(H(Hp\\x18\\xb0\\xd0\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x18\\xb0\\xd0(Hp\\x18(H\\x18X\\x88\\x18X\\x88\\x18X\\x88\\x18X\\x88\\x18X\\x88\\x18(H(Hp\\x18\\xb0\\xd0\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x18\\xb0\\xd0(Hp\\x00\\x00\\x00 x\\xd0@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8 x\\xd0\\x00\\x00\\x00(Hp\\x18\\xb0\\xd0\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x18\\xb0\\xd0\\x18\\xb0\\xd0(Hp\\x00\\x00\\x00@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8\\x00\\x00\\x00(Hp\\x18\\xb0\\xd0\\x18\\xb0\\xd0\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18(H\\x18(H(Hp\\x00\\x00\\x00@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8\\x00\\x00\\x00(Hp\\x18(H\\x18(H\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18(H\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\x18(H\\x18X\\x88\\x18X\\x88\\x18X\\x88\\x18X\\x88\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18X\\x88 x\\xd0@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8 x\\xd0\\x18X\\x88\\xff\\xff\\xff\\xff\\xff\\xff\\x18X\\x88\\x08\\xe0\\xf8\\x08\\xe0\\xf8\\x08\\xe0\\xf8\\x18X\\x88\\x18X\\x88\\x18X\\x88\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00 x\\xd0@\\xc0\\xf8\\xf8\\xe8\\xe8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8\\xf8\\xe8\\xe8@\\xc0\\xf8 x\\xd0\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18X\\x88\\x18\\xb0\\xd0\\x08\\xe0\\xf8\\x08\\xe0\\xf8\\x08\\xe0\\xf8\\x18X\\x88\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x08\\xe0\\xf8\\x08\\xe0\\xf8\\x00\\x00\\x00@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8@\\xc0\\xf8\\x00\\x00\\x00\\x08\\xe0\\xf8\\x08\\xe0\\xf8\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18X\\x88\\x18\\xb0\\xd0\\x08\\xe0\\xf8\\x18X\\x88\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18(H\\x18(H\\x18\\xb0\\xd0\\x00\\x90\\xf8\\x00\\x90\\xf8@\\xc0\\xf8\\x18X\\x88@\\xc0\\xf8\\x00\\x90\\xf8\\x00\\x90\\xf8\\x18\\xb0\\xd0\\x18(H\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18X\\x88\\x18\\xb0\\xd0\\x18\\xb0\\xd0\\x18X\\x88\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00(Hp\\x00\\x90\\xf8\\x18X\\x88\\x18X\\x88 x\\xd0\\x00\\x90\\xf8\\x00\\x90\\xf8\\x00\\x90\\xf8 x\\xd0\\x18X\\x88\\x18X\\x88\\x00\\x90\\xf8\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18X\\x88\\x18X\\x88\\x18\\xb0\\xd0\\x18\\xb0\\xd0\\x18X\\x88\\xff\\xff\\xff\\x00\\x00\\x00(Hp\\x00\\x90\\xf8\\x00\\x90\\xf8 x\\xd0\\x18X\\x88\\x18(H\\x18(H\\x18(H\\x18X\\x88 x\\xd0\\x00\\x90\\xf8\\x00\\x90\\xf8(Hp\\x18(H\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18X\\x88\\x18X\\x88\\x18\\xb0\\xd0\\x18X\\x88\\xff\\xff\\xff\\x18(H\\x18X\\x88\\x00\\x90\\xf8\\x00\\x90\\xf8\\x80\\xb0\\xc0\\xd0\\xe8\\xf8\\xd0\\xe8\\xf8\\xd0\\xe8\\xf8\\x80\\xb0\\xc0\\x00\\x90\\xf8\\x00\\x90\\xf8\\x18X\\x88(Hp\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x18X\\x88\\x18X\\x88\\x18X\\x88\\x00\\x00\\x00 x\\xd0\\x00\\x90\\xf8\\x80\\xb0\\xc0\\xd0\\xe8\\xf8\\xd0\\xe8\\xf8\\xd0\\xe8\\xf8\\xd0\\xe8\\xf8\\xd0\\xe8\\xf8\\x80\\xb0\\xc0\\x00\\x90\\xf8 x\\xd0\\x18(H\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00 x\\xd0 x\\xd0\\x80\\xb0\\xc0\\x80\\xb0\\xc0\\xd0\\xe8\\xf8\\xd0\\xe8\\xf8\\xd0\\xe8\\xf8\\x80\\xb0\\xc0\\x80\\xb0\\xc0 x\\xd0 x\\xd0\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00 x\\xd0\\x18X\\x88\\x80\\xb0\\xc0\\x80\\xb0\\xc0\\x80\\xb0\\xc0\\x80\\xb0\\xc0\\x80\\xb0\\xc0\\x18X\\x88 x\\xd0\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00(Hp\\x00\\x00\\x00\\x18X\\x88\\x18X\\x88\\x18X\\x88\\x18X\\x88\\x18X\\x88\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff'\n\nsamp1 = np.frombuffer(samp1, np.uint8).reshape(24,24,3)\nsamp2 = np.frombuffer(samp2, np.uint8).reshape(24,24,3)\n\n\n# Fashion MNIST data\nclass Split(enum.Enum):\n    TRAIN = 0\n    TEST = 1\n\ndef load_fmnist(split=Split.TRAIN):\n    _URL = \"https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/\"\n    _URLS = ({\n        Split.TRAIN: [\n            _URL+\"train-images-idx3-ubyte.gz\",\n            _URL+\"train-labels-idx1-ubyte.gz\"],\n        Split.TEST: [\n            _URL+\"t10k-images-idx3-ubyte.gz\",\n            _URL+\"t10k-labels-idx1-ubyte.gz\"]\n    })[split]\n    \n    data = [gzip.decompress(urllib.request.urlopen(url).read()) for url in _URLS]\n    images, labels = [\n        np.frombuffer(d, dtype=np.uint8, offset=offset) \n        for d, offset in zip(data, [16, 8])]\n    return images, labels\n\nclass FMNISTDataset(Dataset):\n    \"\"\"Fashion MNIST dataset\"\"\"\n    def __init__(self, split=Split.TRAIN):\n        self.sz = 28\n        self.y_name = [\n            \"T-shirt / top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n            \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n        self.xs, self.ys = load_fmnist(split)\n        self.xs = self.xs.reshape(len(self.ys), self.sz, self.sz)\n        \n        self.xs = batch_tfm(self.xs) #/ 2.\n        self.xs = F.pad(self.xs, (2, 2, 2, 2), value = self.xs.min())\n        self.ys = torch.from_numpy(self.ys).to(torch.int32)\n\n    def __len__(self):\n        return len(self.xs)\n\n    def __getitem__(self, idx):\n        x = self.xs[idx]\n        y = self.ys[idx]\n        return (x, y)\n\n\n\n\nCode (Utils)\n# Assumption: input array of uint8\ndef tfm(arr):\n    \"\"\"Transforms np.array of uint8 to torch.Tensor of -1 ~ 1\"\"\"\n    if isinstance(arr, np.ndarray):\n        if arr.ndim == 2: arr = arr[:, :, None]\n        arr = torch.from_numpy(arr.transpose((2, 0, 1)))\n    return 2 * (arr.float() / 255. - 0.5)\n\nbatch_tfm = lambda arrs: torch.stack([tfm(arr) for arr in arrs])\n\ntfm_rev = lambda t: (t / 2 + 0.5).clamp(0, 1)\n\n\n# Image utils\ndef show_images(imgs, nrows=1, ncols=None, \n                suptitle=None, titles=None, figsize=None, **kwargs):\n    \"\"\"Show all `imgs` in subplots of `nrows` x `ncols`\"\"\"\n    if ncols is None: ncols = int(math.ceil(len(imgs)/nrows))\n    if titles is None: titles = [None]*len(imgs)\n    if figsize is None: figsize = [ncols*3, nrows*3]\n    if suptitle: figsize[1] += 0.6\n    fig, axs = plt.subplots(nrows, ncols, figsize=figsize, **kwargs)\n    if suptitle: fig.suptitle(suptitle, fontsize=20)\n    if nrows*ncols == 1: axs = np.array([axs])\n    for img,t,ax in zip(imgs, titles, axs.flat): show_image(img, ax=ax, title=t)\n\ndef show_image(img, ax=None, title=None, **kwargs):\n    \"\"\"Show an `img` on `ax`\"\"\"\n    # Handle pytorch axis order\n    if all(hasattr(img, p) for p in ('data','cpu','permute')):\n        img = img.data.cpu()\n        if img.shape[0]<5: img=img.permute(1,2,0)\n    elif not isinstance(img,np.ndarray): img=np.array(img)\n        \n    # Handle 1-channel images\n#     if img.shape[-1]==1: img,kwargs['cmap'] = img[...,0],'gray'\n    if img.shape[-1]==1: img = img[...,0]\n        \n    # Handle value range with heuristics (-1 ~ 1 or 0 ~ 255)\n    vmin = img.min(); vmax = img.max()\n    if vmin < 0 and vmax < 5: img = tfm_rev(img) \n\n    if ax is None: ax = plt\n    ax.imshow(img, **kwargs)\n    if title is not None: ax.set_title(title)\n    ax.axis('off')\n    return ax\n\ndef animate_images(imgs, nrows, suptitle=None,\n                   suffixes=None, figsize=None, **kwargs):\n    \"\"\"Create an animated img object\n    \n    Args:\n        imgs: Temporal list of image batch (T x B x C x H x W)\n        nrows: The number of rows of image grid (nrows x B//nrows)\n        suptitle: Title of the animated image grid\n        suffixes: T suffixes appended to `suptitle`\n        figsize: Figure size of the animated image grid\n        \n    Returns:\n        ani (matplotlib.animation.FuncAnimation)\n    \"\"\"\n    ncols = imgs.shape[1]//nrows\n    imgs = np.moveaxis(tfm_rev(imgs).numpy(), 2, 4)\n    if figsize is None: figsize = [ncols*7//5, nrows]\n    if suptitle: figsize[1] += 0.6\n    fig, axs = plt.subplots(nrows, ncols, figsize=figsize, **kwargs)\n    if suptitle: fig.suptitle(suptitle)#, fontsize=20)\n    if nrows*ncols == 1: axs = np.array([axs])\n    axs = axs.reshape(nrows, -1)\n    \n    def animate(i, imgs):\n        print(f'Processing frame {i+1} / {len(imgs)}', end='\\r')\n        plots = []\n        if suffixes: fig.suptitle(f'{suptitle} {suffixes[i]}')\n        for row in range(nrows):\n            for col in range(ncols):\n                axs[row, col].clear()\n                axs[row, col].axis('off')\n                plots.append(\n                    axs[row, col].imshow(imgs[i,(row*ncols)+col]))\n        return plots\n    ani = FuncAnimation(fig, animate, fargs=[imgs], interval=100, \n                        blit=False, repeat=True, frames=len(imgs)) \n    plt.close()\n    return ani\n\n\ndef pbar(iterable, extras={}, min_interval=1):\n    \"\"\"Create a wrapped `iterable` to show a progress bar\n    \n    Args:\n        iterable (Iterable): Iterable collection of objects\n        extras (Dict): 'key: value' will be shown beside the progress bar\n        min_interval (int): Stride of iteration to update the progress bar\n    \"\"\"\n    total = len(iterable)\n    \n    def printProgress(i):\n        extras_str = ', '.join([f'{k}: {v}' for k, v in extras.items()])\n        if extras_str: extras_str = f'({extras_str})'\n        percent = '{0:.1f}'.format(100*(i/total))\n        bar_max_length = 40\n        bar_length = math.ceil(bar_max_length*i/total)\n        bar = '█' * bar_length + '-' * (bar_max_length - bar_length)\n        progress_str = f'|{bar}| {percent}% {extras_str}'\n        print(progress_str, end = '\\r')\n        return len(progress_str)\n\n    print_length = printProgress(0)\n    for i, item in enumerate(iterable):\n        yield item\n        if i % min_interval == 0:\n            print(' ' * print_length, end = '\\r')\n            print_length = printProgress(i + 1)\n    print(' ' * print_length, end = '\\r')\n    print_length = printProgress(total)\n    print()\n    \n    \n# Suppress warnings for code output visibility\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Generated figure titles\nsuptitles = [\n    'Images of forward diffusion process',\n    'Images of reverse diffusion process',\n    'Generated fashion images with DDPM model',\n    'Generated {} images',\n    'Generated {} images with classifier-free guidance',\n    'Progress of image generation with DDIM',\n    'Progress of image generation with DDIM and stride = 20',\n]\n\ntitle_lists = [\n    [f'$x_{{{str(x)}}}$' for x in (0, 200, 400, 600,  800, 1000)],\n    [f'$x_{{{str(x)}}}$' for x in (1000, 800, 600, 400,  200, 0)],\n    [], [], [], [], []\n]"
  },
  {
    "objectID": "posts/diffusion-model/diffusion.html#theoretical-process-in-diffusion-based-models",
    "href": "posts/diffusion-model/diffusion.html#theoretical-process-in-diffusion-based-models",
    "title": "Diffusion based Generative Model",
    "section": "Theoretical process in diffusion-based models",
    "text": "Theoretical process in diffusion-based models\n\nTo understand the diffusion model, it’s necessary to get a grasp on the diffusion process, a theoretical foundation of the model. In short, the diffusion process is a sequential process that transforms the data closer to a noise with a lot of slight-update steps. The idea of mapping data to a noise has some similarities with the other generative models.\n\\[\n\\begin{gathered}\nx_t = \\alpha_t x_{t-1} + \\beta_t \\epsilon_{t} \\\\\n\\tiny where \\ \\ \\alpha_t < 1, \\alpha_t^2 + \\beta_t^2 = 1,\\epsilon_{t} \\sim Normal(0, I) \\ \\ in \\ DDPM\n\\end{gathered}\n\\]\nThe equation above shows how the data \\(x\\) is transformed in each step. As the noise \\(\\epsilon\\) is added to the data \\(x\\) that is being scaled down (\\(\\alpha < 1\\)), \\(x_T\\) and \\(\\epsilon\\) will be indistinguishable from each other after a lot of steps T. This is called forward diffusion process.\nHowever, the generative model should be able to generate the data (not the noise), which means that reverse diffusion process is used to generate a sample from the noise distribution. (If \\(x_T\\) is viewed as a latent variable, it is somewhat similar to other generative models)\n\\[\nx_{t-1} = \\frac{1}{\\alpha_t} (x_t - \\beta_t \\epsilon_{t})\n\\]\n\\(x_T\\) is equivalent to a noise and can be sampled from a normal distribution. Therefore, if \\(\\epsilon\\) can be predicted from \\(x_t\\), the reverse step can be repeated a lot of times to generate the data \\(x_0\\) from the sampled noise \\(x_T\\). Here, neural network model comes in to approximate the noise \\(\\epsilon\\) given \\(x_t\\).\n\\[\n\\epsilon = NeuralNetwork(x_t, t)\n\\]\n\n\n\nDetails (Diffusion process)\n\nAlthough the diffusion process is shown in a linear form above, it doesn’t have to be. The more general (or right) way of describing the diffusion process is with a Markov diffusion kernel \\(K\\) [1]:\n\\[\nq(x_t | x_{t-1}) = K_\\pi(x_t | x_{t-1}; \\beta_t^2) \\\\\n\\] \\[\n\\beta^2 := (Diffusion \\ \\ rate) \\\\\n\\] \\[\nq(x_0) := (Data \\ \\ distribution) \\\\\n\\] \\[\n\\pi \\simeq q(x_T) := (Analyticall \\ \\ tractable \\ \\ simple  \\ \\ distribution) \\\\\n\\]\nFor continuous (or almost continous) data, Gaussian distribution is usually chosen as a simple distribution because of its good properties. One of them in case of a diffusion process is that the forward and reverse diffusion kernel become the same gaussian form when the diffusion rate \\(\\beta_t\\) is chosen small.\n\\[\nq(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t^2}x_{t-1}, \\beta_t^2I) \\\\\n\\] \\[\np(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; f_\\mu(x_t, t), f_\\Sigma(x_t, t)) \\\\\n\\] \\[\nf_s := (Approximate \\ \\  function \\ \\  of \\ \\ s)\n\\]\n\n\n\nCode (Diffusion process)\n# construct DDPM linear noise schedule (alphas, betas)\ntimesteps = 1000\nbeta_sqr_1 = 1e-4\nbeta_sqr_T = 0.02\n\nbetas_sqr_t = torch.linspace(beta_sqr_1, beta_sqr_T, timesteps)\n# alphas_sqr_t = 1 - betas_sqr_t\n\n# alphas_sqr_bar_t = torch.cumsum(alphas_sqr_t.log(), dim=0).exp()\n\nbetas_t = betas_sqr_t.sqrt()\n# alphas_t = alphas_sqr_t.sqrt()\n# alphas_bar_t = alphas_sqr_bar_t.sqrt()\n\n\ndef forward_step(x_t, beta, noise=None):\n    \"\"\"Apply a forward diffusion step to `x_t` with `beta` and `noise`\"\"\"\n    if noise is None:\n        noise = torch.randn_like(x_t)\n    alpha = (1 - beta ** 2).sqrt()\n    return alpha * x_t + beta * noise\n\ndef reverse_step(x_t, beta, noise):\n    \"\"\"Apply a reverse diffusion step to `x_t` with `beta` and `noise`\"\"\"\n    alpha = (1 - beta ** 2).sqrt()\n    return (x_t - beta * noise) / alpha\n\ndef forward_t_steps(x_0, t, betas, return_series=False):\n    \"\"\"Apply a forward diffusion step `t` times to `x_0` with `betas`\n    \n    Args:\n        x_0: Initial data\n        t: The number of forward diffusion steps\n        betas: betas[0] ~ betas[t-1] are used for each forward step\n        return_series (bool):\n            If `True`, return all intermediate xs and noises during the diffusion\n            If `False, return the final diffused data\n            \n    Returns:\n        (t+1 diffused data, t added noises) OR\n        final diffused data\n    \"\"\"\n    xs = [x_0]\n    noises = []\n    \n    x_t = x_0\n    for i in range(t):\n        noise = torch.randn_like(x_t)\n        x_t = forward_step(x_t, betas[i], noise)\n        if return_series:\n            noises.append(noise)\n            xs.append(x_t)\n        \n    out = (xs, noises) if return_series else x_t\n    return out\n\ndef reverse_t_steps(x_T, t, betas, noises, return_series=False):\n    \"\"\"Apply a reverse diffusion step `t` times to `x_T` with `betas` and `noises`\n    \n    Args:\n        x_T: Initial noise or diffused data\n        t: The number of reverse diffusion steps\n        betas: betas[t-1] ~ betas[0] are used for each reverse step\n        noises: noises[t-1] ~ noises[0] are eliminated from data in each step\n        return_series (bool):\n            if `True`, return all intermediate xs\n            if `False, return the final denoised data\n        \n    Returns:\n        t+1 denoised data OR final denoised data\n    \"\"\"\n    xs = [x_T]\n    \n    x_t = x_T\n    for i in range(t-1, -1, -1):\n        x_t = reverse_step(x_t, betas[i], noises[i])\n        if return_series:\n            xs.append(x_t)\n        \n    out = xs if return_series else x_t\n    return out\n\n\n\n\n\nxs, epsilons = forward_t_steps(tfm(samp1), 1000, betas_t, True)\nshow_images(xs[::200], 1, 6, titles=title_lists[0], suptitle=suptitles[0])\n\n\n\n\n\n\n\n\ndenoised_xs = reverse_t_steps(xs[-1], 1000, betas_t, epsilons, True)\nshow_images(denoised_xs[::200], 1, 6, titles=title_lists[1], suptitle=suptitles[1])"
  },
  {
    "objectID": "posts/diffusion-model/diffusion.html#neural-network-model",
    "href": "posts/diffusion-model/diffusion.html#neural-network-model",
    "title": "Diffusion based Generative Model",
    "section": "Neural network model",
    "text": "Neural network model\nAs mentioned above, the noise \\(\\epsilon\\) is approximately predicted by the neural network. One advantage of the diffusion model is that it has a freedom to choose any neural network architecture as long as its output \\(\\epsilon\\) is of the same size as its input \\(x\\).\nWith the architecture freedom, the typical choice is the U-net architecture because the skip-connection from the downsampling path (data encoder) to the upsampling path (data decoder) is proven to work well on a similar task.\n\n\n\nUnet architecture (Source: ZSL)"
  },
  {
    "objectID": "posts/diffusion-model/diffusion.html#noise-scheduler",
    "href": "posts/diffusion-model/diffusion.html#noise-scheduler",
    "title": "Diffusion based Generative Model",
    "section": "Noise scheduler",
    "text": "Noise scheduler\nThe noise scheduler is defined with the parameters of the diffusion process. It is used to generate a corrupted data sample \\(x_t\\) from its clean version \\(x_0\\) for NN model training, and to generate a data sample \\(x_0\\) from noise \\(\\epsilon\\) by applying the reverse-diffusion (denoise) process with the trained NN model.\n\n\\(T\\): the maximum number of steps of the diffusion process\n\\(\\beta_t\\): The strength of a noise \\(\\epsilon\\) added in each diffusion step\n(\\(\\alpha_t\\) can be usually derived from \\(\\beta_t\\))\n\nAs long as \\(\\alpha_t < 1\\) is satisfied, the diffusion process will transform the data \\(x\\) to the normally distributed noise \\(\\epsilon\\). However, it usually has the following properties to make it easier for a neural network to predict the noise of a single diffusion step.\n\n\\(\\alpha_t\\) is a decreasing series from ~1 to ~0\n\\(\\prod_{i=1}^{t}\\alpha_i = \\alpha_1 \\times \\alpha_2 \\times \\cdots \\times \\alpha_t = \\bar{\\alpha}_t\\) shows a linear drop around \\(t \\approx \\frac{T}{2}\\) and an almost flat line with a very little decrease around \\(t \\approx 0\\) and \\(t \\approx T\\)\n\n\n\nDetails (Noise scheduler)\n\nTo be precise, noise scheduler is defined with the transformation of foward/reverse diffusion steps. With the defined diffusion process and given parameters, it should be able to transform data to a noise and denoise the noise back to data.\n\n\n\nCode (U-net)\nclass UnetModel(nn.Module):\n    \"\"\"U-net model\"\"\"\n    \n    def __init__(self, in_channels, out_channels, block_out_channels, max_timestep, \n                 norm_groups=None, norm_eps=1e-6, \n                 block_num_layers=2, temb_channels=None, \n                 emb_type='shift', act_fn=nn.ReLU(), num_classes=None):\n        \"\"\"\n        Args:\n            in_channels: Number of input channels\n            out_channels: Number of output channels\n            block_out_channels: Number of output channels of each UnetDownBlock\n            max_timestep: Maximum timestep (NoiseScheduler.max_t)\n            norm_groups: Number of channel groups used in GroupNorm\n            norm_eps: Value for stability in GroupNorm\n            block_num_layers: Number of Residual blocks to be used in downsampling\n                              (block_num_layers + 1 Residual blocks in upsampling)\n            temb_channels: Number of timestep embedding channels\n                           If None, default to 4 *  block_out_channels[0]\n            emb_type: 'shift': additive, 'scale_shift': additive + multiplicative\n                If 'shift', h -> h + emb\n                If 'scale_shift', h -> h * (1 + emb[:n/2]) + emb[n/2:]\n            act_fn: Activation function to be used\n            num_classes: If not None, number of possible conditions\n        \"\"\"\n        super().__init__()\n\n        if not temb_channels: # Timestep embedding channels\n            temb_channels = 4 * block_out_channels[0]\n        if not norm_groups:\n            norm_groups = block_out_channels[0] // 4\n        if len(block_out_channels) > 0:\n            block_out_channels += [block_out_channels[-1]]\n\n        # Class condition embedding\n        cemb_channels = 0\n        self.cemb_module = None\n        if num_classes is not None:\n            cemb_channels = temb_channels//4\n            self.cemb_module = nn.Embedding(num_classes, cemb_channels)\n\n        # Timestep embedding module\n        self.temb_module = \\\n            TimestepEmbedding(temb_channels//2, temb_channels, \n                              act_fn=act_fn, max_timestep=max_timestep)\n\n        # First convolution\n        self.conv_in = nn.Sequential(\n            nn.Conv2d(in_channels, block_out_channels[0], 3, stride=1, padding=1)\n        )\n        \n        # Downsample and Upsample blocks\n        self.down_blocks = nn.ModuleList()\n        self.up_blocks = nn.ModuleList()\n\n        # Last convolution\n        self.conv_out = nn.Sequential(\n            nn.GroupNorm(norm_groups, block_out_channels[0], norm_eps, affine=True),\n            act_fn,\n            nn.Conv2d(block_out_channels[0], out_channels, 3, stride=1, padding=1)\n        )\n\n        # Downsample blocks\n        out_channels = block_out_channels[0]\n        for i in range(len(block_out_channels)):\n            downsample = False if i == len(block_out_channels) - 1 else True\n            \n            in_channels = out_channels\n            out_channels = block_out_channels[i]\n\n            self.down_blocks.append(\n                UnetDownBlock(\n                    in_channels,\n                    out_channels,\n                    temb_channels + cemb_channels,\n                    norm_groups,\n                    norm_eps,\n                    num_layers=block_num_layers,\n                    emb_type=emb_type,\n                    act_fn=act_fn,\n                    downsample=downsample\n                )\n            )\n\n        # Upsample blocks\n        reversed_block_out_channels = list(reversed(block_out_channels))\n        out_channels = reversed_block_out_channels[0]\n        for i in range(len(reversed_block_out_channels)):\n            upsample = False if i == len(reversed_block_out_channels) - 1 else True\n            \n            in_channels = out_channels\n            out_channels = reversed_block_out_channels[i]\n            down_channel_idx = min(i + 1, len(reversed_block_out_channels) - 1)\n            down_block_in_channels = reversed_block_out_channels[down_channel_idx]\n\n            self.up_blocks.append(\n                UnetUpBlock(\n                    in_channels,\n                    out_channels,\n                    down_block_in_channels,\n                    temb_channels + cemb_channels,\n                    norm_groups,\n                    norm_eps,\n                    num_layers=block_num_layers + 1,\n                    emb_type=emb_type,\n                    act_fn=act_fn,\n                    upsample=upsample\n                )\n            )\n\n    def forward(self, x, t, c=None):\n        \"\"\"\n        Args:\n            x: Input data\n            t: Input timestep\n            c: Optional condition (class) input\n        \"\"\"\n        # Create an embedding of timestep (+ class)\n        emb = self.temb_module(t)\n        if self.cemb_module is not None:\n            cemb = self.cemb_module(c)\n            emb = torch.cat((emb, cemb), dim=1)\n\n        # Input convolution\n        h = self.conv_in(x)\n\n        # Downsampling path\n        h_all_states = (h, )\n        for down_block in self.down_blocks:\n            h, h_states = down_block(h, emb)\n\n            h_all_states += h_states\n            \n        # Upsampling path\n        for up_block in self.up_blocks:\n            # Skip-connections\n            skip_h_states = h_all_states[-up_block.num_layers:]\n            h_all_states = h_all_states[:-up_block.num_layers]\n\n            h = up_block(h, skip_h_states, emb)\n\n        out = self.conv_out(h)\n\n        return out\n\n    \nclass TimestepEmbedding(nn.Module):\n    \"\"\"Timestep embedding module\"\"\"\n    \n    def __init__(self, hidden_dim, embedding_dim, act_fn, \n                 max_timestep=1e3, scale=math.pi/2):\n        \"\"\"\n        Args:\n            hidden_dim: Dimension of intermediate hidden layer\n            embedding_dim: Embedding dimension\n            act_fn: Activation function to be used\n            max_timestep: Maximum timestep (NoiseScheduler.max_t)\n        \"\"\"\n        super().__init__()\n        \n        self.h_dim = hidden_dim #embedding_dim // 4\n        self.emb_dim = embedding_dim\n        self.max_t = max_timestep\n        self.scale = scale\n        \n        self.layers = nn.Sequential(*[\n            nn.Linear(self.h_dim, self.emb_dim),\n            act_fn,\n            nn.Linear(self.emb_dim, self.emb_dim)\n        ])\n\n    def forward(self, timesteps):\n        \"\"\"\n        Args:\n            timesteps: Batch of timesteps\n            \n        Returns:\n            Batch of timestep embeddings\n        \"\"\"\n        # Create const-like, linear-like and random-like embeddings\n        half_h_dim = self.h_dim // 2\n        exponent = -math.log(10 * self.max_t) / half_h_dim * \\\n            torch.arange(0, half_h_dim, device=timesteps.device)\n        emb = torch.exp(exponent) * self.scale\n        emb = timesteps[:, None].float() * emb[None, :]\n\n        emb = torch.cat([torch.cos(emb), torch.sin(emb)], dim=-1)\n        \n        temb = self.layers(emb)\n        \n        return temb\n\n    \nclass UnetDownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, emb_channels, norm_groups,\n                 norm_eps=1e-6, num_layers=2, emb_type='shift', \n                 act_fn=nn.ReLU(), downsample=True, output_scale=1.0):\n        \"\"\"\n        Args:\n            in_channels: Number of input channels\n            out_channels: Number of output channels\n            emb_channels: Number of embedding channels\n                          (timestep or timestep + condition embedding)\n            norm_groups: Number of channel groups used in GroupNorm\n            norm_eps: Value for stability in GroupNorm\n            num_layers: Number of Residual blocks to be used\n            emb_type: 'shift': additive, 'scale_shift': additive + multiplicative\n                If 'shift', h -> h + emb\n                If 'scale_shift', h -> h * (1 + emb[:n/2]) + emb[n/2:]\n            act_fn: Activation function to be used\n            downsample: whether or not to apply downsampling \n        \"\"\"\n        super().__init__()\n\n        self.res_blocks = nn.ModuleList()\n        self.downsample_blocks = nn.ModuleList()\n\n        # Residual blocks\n        for i in range(num_layers):\n            in_ch = in_channels\n            out_ch = in_channels\n\n            self.res_blocks.append(\n                ResBlock(in_ch, out_ch, emb_channels, norm_groups, norm_eps, \n                         emb_type, act_fn, init_zero=True, \n                         output_scale=output_scale)\n            )\n        \n        # Downsampling modules\n        if downsample:\n            self.downsample_blocks.append(\n                DownSample2D(in_channels, out_channels, \n                             norm_groups, norm_eps, act_fn, output_scale)\n            )\n\n    def forward(self, x, emb):\n        # Gather hidden states for skip-connections\n        h_states = ()\n\n        h = x\n        for block in self.res_blocks:\n            h = block(h, emb)\n            h_states += (h, )\n        \n        for block in self.downsample_blocks:\n            h = block(h)\n            h_states += (h, )\n\n        return h, h_states\n\n\nclass UnetUpBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, down_in_channels, emb_channels, \n                 norm_groups, norm_eps=1e-6, num_layers=3, emb_type='shift', \n                 act_fn=nn.ReLU(), upsample=True, output_scale=1.0):\n        \"\"\"\n        Args:\n            in_channels: Number of input channels\n            out_channels: Number of output channels\n            down_in_channels: Number of skip-connection channels\n            emb_channels: Number of embedding channels\n                          (timestep or timestep + condition embedding)\n            norm_groups: Number of channel groups used in GroupNorm\n            norm_eps: Value for stability in GroupNorm\n            num_layers: Number of Residual blocks to be used\n            emb_type: 'shift': additive, 'scale_shift': additive + multiplicative\n                If 'shift', h -> h + emb\n                If 'scale_shift', h -> h * (1 + emb[:n/2]) + emb[n/2:]\n            act_fn: Activation function to be used\n            upsample: whether or not to apply downsampling \n        \"\"\"\n        super().__init__()\n        \n        self.num_layers = num_layers\n\n        self.res_blocks = nn.ModuleList()\n        self.upsample_blocks = nn.ModuleList()\n        \n        # Residual blocks\n        for i in range(num_layers):\n            skip_channels = down_in_channels\n            in_ch = (out_channels) + skip_channels\n            out_ch = out_channels \n            \n            self.res_blocks.append(\n                ResBlock(in_ch, out_ch, emb_channels, norm_groups, norm_eps, \n                         emb_type, act_fn, output_scale=output_scale)\n            )\n        \n        # Upsampling modules\n        if upsample:\n            self.upsample_blocks.append(Upsample2D(out_ch, down_in_channels))\n\n\n    def forward(self, x, skip_states, emb):\n        h = x\n        for block in self.res_blocks:\n            # skip connection from the downsampling path of Unet\n            skip_h = skip_states[-1]\n            skip_states = skip_states[:-1]\n            h = torch.cat((h, skip_h), dim=1)\n\n            h = block(h, emb)\n\n        for block in self.upsample_blocks:\n            h = block(h)\n        \n        return h\n\n    \nclass DownSample2D(nn.Module):\n    \"\"\"2D downsampling module with residual-like layers\"\"\"\n    \n    def __init__(self, in_channels, out_channels, norm_groups,\n                 norm_eps=1e-6, act_fn=nn.ReLU(), output_scale=1.0):\n        \"\"\"\n        Args:\n            in_channels: Number of input channels\n            out_channels: Number of output channels\n            norm_groups: Number of channel groups used in GroupNorm\n            norm_eps: Value for stability in GroupNorm\n            act_fn: Activation function to be used\n        \"\"\"\n        super().__init__()\n        \n        self.output_scale = output_scale\n        self.act_fn = act_fn\n\n        # 1st path of 3x3 convolutions\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, \n                               stride=2, padding=1, bias=False)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, \n                               stride=1, padding=1, bias=False)\n        \n        # 2nd path of polling and 1x1 convolution\n        self.pool  = nn.AvgPool2d(2, 2)\n        self.conv3 = nn.Conv2d(in_channels, out_channels, 1, 1, bias=False)\n        \n        self.norm1 = nn.GroupNorm(norm_groups, in_channels, norm_eps, affine=True)\n        self.norm2 = nn.GroupNorm(norm_groups, out_channels, norm_eps, affine=True)\n        \n        # Init residual path to zero\n        nn.init.zeros_(self.conv2.weight)\n        \n    def forward(self, x):\n        x = self.act_fn(self.norm1(x))\n        \n        # 1st path\n        h1 = self.conv1(x)\n        h1 = self.act_fn(self.norm2(h1))\n        h1 = self.conv2(h1)\n        \n        # 2nd path\n        h2 = self.conv3(self.pool(x))\n        \n        return (h1 + h2) / self.output_scale\n\n\nclass Upsample2D(nn.Module):\n    \"\"\"2D upsampling module\"\"\"\n    \n    def __init__(self, in_channels, out_channels, \n                 scale=2.0, interpolate_mode='bilinear'):\n        \"\"\"\n        Args:\n            in_channels: Number of input channels\n            out_channels: Number of output channels\n            scale: Upsampling ratio\n            interpolation_mode: Upsampling interpolation mode\n        \"\"\"\n        super().__init__()\n        \n        self.scale = scale\n        self.interpolate_mode = interpolate_mode\n        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n\n    def forward(self, x):\n        h = F.interpolate(x, scale_factor=self.scale, mode=self.interpolate_mode)\n        h = self.conv(h)\n        return h\n\n    \nclass ResBlock(nn.Module):\n    \"\"\"Residual block\"\"\"\n    \n    def __init__(self, in_channels, out_channels, emb_channels, norm_groups, \n                 norm_eps=1e-6, emb_type='shift', act_fn=nn.ReLU(),\n                 skip_time_act=False, init_zero=False, output_scale=1.0):\n        \"\"\"\n        Args:\n            in_channels: Number of input channels\n            out_channels: Number of output channels\n            emb_channels: Number of embedding channels\n                          (timestep or timestep + condition embedding)\n            norm_groups: Number of channel groups used in GroupNorm\n            norm_eps: Value for stability in GroupNorm\n            emb_type: 'shift': additive, 'scale_shift': additive + multiplicative\n                If 'shift', h -> h + emb\n                If 'scale_shift', h -> h * (1 + emb[:n/2]) + emb[n/2:]\n            act_fn: Activation function to be used\n            skip_time_act: whether or not to skip pre-activation of input embedding\n            init_zero: whether or not to init last module of conv path to zero\n        \"\"\"\n        super().__init__()\n\n        self.emb_type = emb_type\n        self.act_fn = act_fn\n        self.skip_time_act = skip_time_act\n        self.output_scale = output_scale\n\n        # Convolution path\n        self.norm1 = nn.GroupNorm(norm_groups, in_channels, norm_eps, affine=True)\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, \n                               stride=1, padding=1, bias=False)\n        self.norm2 = nn.GroupNorm(norm_groups, out_channels, norm_eps, affine=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, \n                               stride=1, padding=1, bias=False)\n\n        if emb_type == 'scale_shift':\n            self.emb_proj = nn.Linear(emb_channels, 2*out_channels)\n        else:\n            self.emb_proj = nn.Linear(emb_channels, out_channels)\n            \n        # Residual path\n        self.conv3 = nn.Identity()\n        if in_channels != out_channels:\n            self.conv3 = nn.Conv2d(in_channels, out_channels, 1, \n                                   stride=1, padding=0, bias=True)\n            \n        if init_zero:\n            nn.init.zeros_(self.conv2.weight)\n\n    def forward(self, x, emb):\n        # Process input embedding into offset (or scale + offset)\n        if self.emb_type is not None:\n            if not self.skip_time_act:\n                emb = self.act_fn(emb)\n            emb = self.emb_proj(emb)[:, :, None, None] # unsqueeze over H x W\n\n            scale = 0\n            offset = emb\n            if self.emb_type == 'scale_shift':\n                scale, offset = torch.chunk(emb, 2, dim=1)\n\n        # Convolution path\n        h = self.act_fn(self.norm1(x))\n        h = self.conv1(h)\n\n        h = h * (1 + scale) + offset\n\n        h = self.act_fn(self.norm2(h))\n        h = self.conv2(h)\n\n        return (self.conv3(x) + h) / self.output_scale\n\n\n\n\nCode (Noise scheduler)\nclass NoiseScheduler:\n    \"\"\"Implementation of forward and reverse diffusion processes\n    \n    Parameters:\n        max_t: maximum possible timestep T\n        betas: A series of stdev of normally distributed noises\n        ts: A series of increasing integers 1 ~ T\n    \n    Methods:\n        add_noise: Apply the forward diffusion step t times\n        denoise: Apply a single reverse diffusion step\n    \"\"\"\n    def __init__(self, max_t, betas):\n        self.max_t = max_t\n        self.betas = betas\n        self.ts = list(range(1, max_t + 1))\n        \n    def _process(self, step_func, iterable):\n        processed_xb = []\n        for step_func_args in iterable:\n            x_t = step_func(*step_func_args)\n            processed_xb.append(x_t)\n        processed_xb = torch.stack(processed_xb)\n        \n        return processed_xb\n    \n    def to(self, device):\n        self.betas = self.betas.to(device)\n        return self\n    \n    def add_noise(self, xb, tb, noiseb):\n        \"\"\"Apply the forward diffusion step `tb` times to `xb` with `noiseb`\n        \n        Args:\n            xb: Batch of data x\n            tb: Batch of timesteps t\n            noiseb: Batch of noises that will be added to `xb`\n        \n        Returns:\n            Batch of diffused data\n        \"\"\"\n        if isinstance(xb, np.ndarray):\n            xb = batch_tfm(xb)\n            \n        if noiseb is None:\n            betasb = (self.betas[:t] for t in tb)\n            diffused_xb = self._process(forward_t_steps, \n                                        zip(xb, betasb))\n        else:\n            diffused_xb = self._process(forward_step, \n                                        zip(xb, self.betas[tb-1], noiseb))\n        return diffused_xb\n    \n    def denoise(self, xb, t, noiseb):\n        \"\"\"Apply a single reverse diffusion step to `xb` with `noiseb` at `t`\n        \n        Args:\n            xb: Batch of diffused data x\n            t: Current timestep of `xb`. This controls the amount of noise to remove.\n            noiseb: Batch of noises to eliminate from `xb`\n        \n        Returns:\n            Batch of data denoised by a single reverse step\n        \"\"\"\n        if isinstance(xb, np.ndarray):\n            xb = batch_tfm(xb)\n        iterable = zip(xb, self.betas[[t-1] * len(xb)], noiseb)\n        denoised_xb = self._process(reverse_step, iterable)\n        return denoised_xb\n\n\n\n\nCode (Diffusion model)\nclass DiffusionModelBase:\n    \"\"\"Diffusion model that can train a noise predictor and generate data\n    \n    Parameters:\n        noise_scheduler (NoiseScheduler): Diffusion process implementation\n        noise_predictor (torch.nn.Module): Noise predicting model\n        optim (torch.optim.Optimizer): Optimiser of `noise_predictor` \n    \n    Methods:\n        train: Train `noise_predictor` with diffused data samples\n        generate: Generate denoised clean data from normally distributed noise\n    \"\"\"\n    def __init__(self, nn_model, noise_scheduler, optimizer=None, device='cuda'):\n        self.device = device\n        self.noise_predictor = nn_model.to(device)\n        self.noise_scheduler = noise_scheduler.to(device)\n        if optimizer is None:\n            self.optim = torch.optim.Adam(self.noise_predictor.parameters(), lr=1e-3)\n        else:\n            self.optim = optimizer\n            \n        \n    def _diffuse_data(self, xb, tb, noiseb):\n        # Add `noiseb` to (`tb` - 1)th diffused data \n        diffused_xb_pre = self.noise_scheduler.add_noise(xb, tb - 1, None)\n        diffused_xb = self.noise_scheduler.add_noise(diffused_xb_pre, tb, noiseb)\n        return diffused_xb\n    \n    def _predict_noise(self, xb, tb, cb):\n        return self.noise_predictor(xb, tb)\n    \n    def train(self, dataloader, n_epoch=5):\n        \"\"\"Train `noise_preditor` on `dataloader`\n        \n        Args:\n            dataloader (torch.utils.data.DataLoader): Iterable training data\n            n_epoch: Number of epochs to train `noise_predictor`\n        \"\"\"\n        max_t = self.noise_scheduler.max_t\n        base_lr = self.optim.param_groups[0]['lr']\n        losses = []\n\n        self.noise_predictor.train()\n#         step = 0\n\n        for ep in range(n_epoch):\n            # Exponential decay learning rate per epoch\n            self.optim.param_groups[0]['lr'] = base_lr/(2**ep)\n\n            extra_info = {'epoch': ep}\n            dl = pbar(dataloader, extra_info, min_interval=2)\n            for xb, yb in dl:   # x: images, y: labels\n#                 self.optim.param_groups[0]['lr'] = base_lr * lr_lambda(step)\n#                 step += 1\n                self.optim.zero_grad()\n                xb = xb.to(self.device)\n\n                # Perturb data\n                noiseb = torch.randn_like(xb)\n                tb = torch.randint(1, max_t + 1, (xb.shape[0],)).to(self.device)\n                \n                diffused_xb = self._diffuse_data(xb, tb, noiseb)\n\n                # Use neural network to predict noise\n                pred_noiseb = self._predict_noise(diffused_xb, tb, yb)\n\n                # Loss between the predicted and true noise\n                loss = F.mse_loss(pred_noiseb, noiseb)\n                loss.backward()\n                self.optim.step()\n                \n                # Show learning-rate and 100-step mean of loss with a progress bar\n                losses.append(loss.item())\n                extra_info['loss'] = np.mean(losses[-100:]).round(4)\n                extra_info['lr'] = self.optim.param_groups[0]['lr']\n\n    @torch.no_grad()\n    def generate(self, n_sample, n_ch, sz, \n                 cb=None, seed=None, return_intermediates=False):\n        \"\"\"Generate data (`n_sample` x `n_ch` x `sz` x `sz`)\n        \n        Args:\n            n_sample: Number of data samples to generate\n            n_ch: Number of channel\n            sz: Height and width\n            cb: Optional batch of condition c\n                (It requires `noise_predictor` to be a conditional model)\n            seed: Optional random seed\n            return_intermediates (bool):\n                if `True`, return all intermediate data during generation process\n                if `False, return the final generated data\n            \n        Returns:\n            (final generated data, (intermediate timesteps, intermediate data)) OR\n            final generated data\n        \"\"\"\n        max_t = self.noise_scheduler.max_t\n        samples = torch.randn(n_sample, n_ch, sz, sz, generator=seed)\n        samples = samples.to(self.device)\n        intermediate_imgs = [samples.detach().cpu()]\n        intermediate_ts = [max_t]\n        \n        self.noise_predictor.eval()\n        \n        extra_info = {}\n        reversed_ts = self.noise_scheduler.ts[::-1]\n        progress = pbar(reversed_ts, extra_info)\n        for i, t in enumerate(progress):\n            tb = torch.tensor([t]*n_sample).to(self.device)\n\n            # Use the trained noise_predictor to predict noise\n            pred_noiseb = self._predict_noise(samples, tb, cb)\n            \n            # Apply a reverse diffusion step (denoise step)\n            samples = self.noise_scheduler.denoise(samples, t, pred_noiseb)\n            \n            # Sample intermiedate data with rate of (max_t // 100)\n            if return_intermediates:\n                if i % (max_t // 100) == 0 or t < 5:\n                    intermediate_imgs.append(samples.detach().cpu())\n                    intermediate_ts.append(t)\n\n        intermediates = (intermediate_ts, torch.stack(intermediate_imgs))\n        return (samples, intermediates) if return_intermediates else samples\n\n    \n# def lr_lambda(current_step):\n#     num_warmup_steps = 350\n#     num_training_steps = 3744\n#     num_cycles = 0.5\n#     if current_step < num_warmup_steps:\n#         return float(current_step) / float(max(1, num_warmup_steps))\n#     progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n#     return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))"
  },
  {
    "objectID": "posts/diffusion-model/diffusion.html#conditional-model-and-guidance-technique",
    "href": "posts/diffusion-model/diffusion.html#conditional-model-and-guidance-technique",
    "title": "Diffusion based Generative Model",
    "section": "Conditional model and Guidance technique",
    "text": "Conditional model and Guidance technique\n\n\n\n\n\nIf there is a mechanism to control the generated data, it will be more useful. One typical method in deep learning is to add additional input data \\(c\\) such that the model generates data relevant to the information \\(c\\) from a random noise \\(z\\).\nAnother common method in a diffusion model is called Guidance. The reverse diffusion step can be viewed as a denoising step: Given a noise-like data \\(x_t\\), find out less noise-like one \\(x_{t-1}\\) by predicting the added noise \\(\\epsilon_t\\) and eliminating it from \\(x_t\\). Guidance changes the noise prediction \\(\\epsilon\\) with the condition information \\(c\\) such that \\(x_{t-1}\\) is closer to the data \\(x\\) relevant to \\(c\\) out of all other possible \\(x\\). Parameter called Guidance scale determines the strength towards the controlled generation.\n\n\n\n\n\nThere are 2 kinds of guidance tecniques:\n\nClassifier-based guidance: It uses a seperately trained classifier model that can predict \\(p(c|x)\\). As it’s trained with \\(x\\) and \\(c\\), it knows about the relationship between \\(x\\) and \\(c\\), and by updating \\(\\epsilon\\) with this knowledge in a specific math formula, it can control the data generation process with \\(c\\) [4].\nClassifier-free guidance: It uses a single conditional diffusion model that takes \\(x\\) and \\(c\\) as inputs. One requirement, unlike the typical conditional model, is that the model should be able to predict unconditionally as well when \\(c\\) is not given. In each reverse diffusion step, the neural network model will predict \\(\\epsilon\\) twice, conditioned on \\(c\\) and unconditionally, the difference between these 2 prediction is used to update \\(\\epsilon\\) [5].\n\nThere could be a question for a classifier-free guidance like “Why is it needed when the conditional model can already generate a controlled data with the condition information \\(c\\)?” It’s true that this technique is not mandatory when a conditional model is trained. But it’s shown that the guidance can additionally improve the generated data quality while sacrificing sample diversity a bit.\n\n\nDetails (Mathematical formulation of guidance technique)\n\nWhen the probabilty follows a normal distribution, its score function is related to a noise as follows: \\[\n\\nabla_{x} log \\ p(x) = \\nabla_{x} (- \\frac{(x-\\mu)^2}{2 \\ \\sigma^2}) = - \\frac{\\epsilon}{\\sigma} \\ \\ \\ \\ \\ (\\epsilon \\sim \\mathcal{N}(0, 1))\n\\]\nUsing this relationship, the classifier-based guidance modifies the noise prediction by matching \\(\\nabla_{x_t} log \\ p_{\\theta, \\phi}(x_t | c)\\) to \\(\\nabla_{x_t} log \\ p_\\theta(x_t)\\) with a separate classifier \\(p_\\phi(c|x)\\).\n\\[\n\\begin{align}\n\\nabla_{x_t} log \\ p_{\\theta, \\phi}(x_t | c) &= \\nabla_{x_t} log \\ p_\\theta(x_t) + \\nabla_{x_t} log \\ p_\\phi(c | x_t) \\ \\ \\ \\ \\ \\because Bayes' \\ theorem \\\\\n    &\\simeq -\\frac{1}{\\sigma} (\\bar{\\epsilon}_t - \\sigma \\cdot \\nabla_{x_t} log \\ p_\\phi(c|x_t)) \\\\\n\\therefore \\tilde{\\epsilon}_t &= \\bar{\\epsilon}_t - s \\cdot \\sigma \\cdot \\nabla_{x_t} log \\ p_\\phi(c|x_t))\n\\end{align}\n\\]\nThe classifier-free guidance uses a single conditional model to model the classifier score function \\(\\nabla log \\ p(c|x)\\). \\[\n\\begin{align}\n\\nabla_{x_t} log \\ p(c|x_t) &= \\nabla_{x_t} log \\ p(x_t|c) - \\nabla_{x_t} log \\ p(x_t) \\ \\ \\ \\ \\ \\because Bayes' \\ theorem \\\\\n\\therefore \\tilde{\\epsilon}_t &= \\bar{\\epsilon}_t(x_t) - s \\cdot \\sigma \\cdot - \\frac{1}{\\sigma} (\\bar{\\epsilon}_t(x_t, c) - \\bar{\\epsilon}_t(x_t)) \\\\\n    &= \\bar{\\epsilon}_t(x_t) + s \\cdot (\\bar{\\epsilon}_t(x_t, c) - \\bar{\\epsilon}_t(x_t))\n\\end{align}\n\\]\n\n\n\nCode (Conditional model and Guidance)\nclass ConditionalDiffusionModel(DiffusionModel):\n    \"\"\"Conditional diffusion model that can train a noise predictor and \n       generate data conditionally\n    \n    Parameters:\n        noise_scheduler (NoiseScheduler): Diffusion process implementation\n        noise_predictor (torch.nn.Model): Noise predicting model\n        optim (torch.optim.Optimizer): Optimiser of `noise_predictor` \n        uncond_label: Integer (>= 0) label to indicate an unconditional input\n                      (20% of input labels(conditions) will be randomly set)\n        g_scale: Optional guidance scale \n                 (if not None, guidance technique is used)\n    \n    Methods:\n        train: Train `noise_predictor` with diffused data samples\n        generate: Generate denoised clean data from normally distributed noise\n    \"\"\"\n    \n    def __init__(self, uncond_label, guidance_scale=None, **kwargs):\n        super().__init__(**kwargs)\n        self.uncond_label = uncond_label\n        self.g_scale = guidance_scale \n    \n    def _predict_noise(self, xb, tb, cb):\n        # Move batch of conditional information to `self.device`\n        cb = cb.to(self.device)\n        \n        if self.noise_predictor.training:\n            # randomly set 20% of condition labels to uncond_label \n            # to train a nn_model to make an unconditional inference\n            idxs = np.random.choice(range(len(cb)), len(cb)//5, replace=False)\n            cb[idxs] = self.uncond_label\n            \n            noise = self.noise_predictor(xb, tb, cb)\n        else:\n            # Predict noise twice: conditional and unconditional inferences\n            if self.g_scale is not None:\n                xb = xb.repeat(2, 1, 1, 1)\n                tb = tb.repeat(2)\n                cb = torch.cat((cb, torch.full_like(cb, self.uncond_label)))\n                \n            noise = self.noise_predictor(xb, tb, cb)\n            \n            if self.g_scale is not None:\n                # Update noise with classifier-free guidance\n                bs = len(noise)//2\n                guidance = noise[:bs] - noise[bs:]\n                noise = noise[bs:] + self.g_scale * guidance\n                \n        return noise\n    \n\n# Conditional diffusion model training on FMNIST dataset\nn_labels = len(dataset.y_name) # Number of labels (conditions)\n\nnn_model = UnetModel(\n    in_channels=1,\n    out_channels=1,\n    block_num_layers=2,\n    block_out_channels=[32, 64, 128],\n    max_timestep=NUM_TIMESTEPS,\n    norm_groups=8,\n    num_classes=(n_labels + 1) # Include unconditional label\n)\n# Init embedding of uncoditional label(n_labels==10) to 0\nnn_model.cemb_module.weight.data[n_labels] = 0\n\n\nCondDDPMModel = ConditionalDiffusionModel(\n    uncond_label = n_labels,\n#     guidance_scale = 3.0,\n    nn_model = nn_model,\n    noise_scheduler = DDPMScheduler\n)\n\nCondDDPMModel.train(dataloader, 5)\n\n\n\n\n\nn_samples = 10\nlabel = 'Ankle boot'\ncondition = dataset.y_name.index(label)\nconds = torch.tensor(condition).repeat_interleave(n_samples)\n\nCondDDPMModel.g_scale = None\ncond_imgs = CondDDPMModel.generate(n_samples, n_ch=1, sz=32, cb=conds)\nshow_images(cond_imgs, 1, 10, suptitle=suptitles[3].format(label))\n\n\n\n\n\n\n\n\nCondDDPMModel.g_scale = 3.0\ncond_guide_imgs = CondDDPMModel.generate(n_samples, n_ch=1, sz=32, cb=conds)\nshow_images(cond_guide_imgs, 1, 10, suptitle=suptitles[4].format(label))"
  },
  {
    "objectID": "posts/diffusion-model/diffusion.html#nobility-vs-memorisation",
    "href": "posts/diffusion-model/diffusion.html#nobility-vs-memorisation",
    "title": "Diffusion based Generative Model",
    "section": "Nobility vs Memorisation",
    "text": "Nobility vs Memorisation\n\n\n\nGenerated pokemon sprites (blue-box: noble+memorised sample, red-box: memorised sample, others: noble sample)\n\n\nThere could be a question,\n“Okay, the diffusion model seems to be able to generate something that looks real, but isn’t it just one of the data the model has already seen? It doesn’t seem to have any special component that helps to create a noble one. How can it be useful if it just copies and pastes one of the data it has seen during training?”\nIt is true that the diffusion model memorises non-diffused data to do a denoising step effectively. The memorisation of a whole sample happens. However, that’s not the only thing that the model learns. Inductive bias of a neural network plus a sauce of randomness helps learn the common parts/characteristics of multiple samples as well. As a result, the model will not only copy the data but also generate a noble new sample. This seems analogus to how human creativity comes out of knowledge with a few memory errors."
  }
]